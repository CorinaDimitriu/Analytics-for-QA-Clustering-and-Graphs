{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1de278ff-9b9a-4b08-a5b3-de05ca14a880",
   "metadata": {},
   "source": [
    "Code for Topic Modelling added over the Question Graph(Graph where we add a Topic node and connect question node to the topic node accordingly) and the Question Paragraph Graph(associates over the QP graph the topics given,to see how distributed and balanced they are)\n",
    "This code was tested locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d970cfe-9dc0-42c1-869b-8fd867e57abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import typing\n",
    "from typing import Union\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"Simple Graphs\")\n",
    "    .config('spark.executor.instances','2')\n",
    "    .config('spark.executor.memory','12G')\n",
    "    .config('spark.executor.cores','4')\n",
    "    .config('spark.driver.memory','6G')\n",
    "    .config('spark.dynamicAllocation.enabled','false') # musai??\n",
    "    .master('spark://master:7077')\n",
    "    .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.4-spark3.5-s_2.12\")\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "from graphframes import GraphFrame\n",
    "\n",
    "OUTPUT_FILE_FOLDER = 'results'\n",
    "\n",
    "TOPIC_MODELING_QUESTION_GRAPH_NAME = 'TPQ Graph'\n",
    "TOPIC_MODELING_QUESTION_PARAGRAPH_GRAPH_NAME = 'TPQP Graph'\n",
    "\n",
    "# TODO:Adjust edge thickness to look better?\n",
    "# EDGE_IS_SUPPORTING_THICKNESS = 2\n",
    "# EDGE_IS_NOT_SUPPORTING_THICKNESS = 2\n",
    "\n",
    "GRAPH_QUESTION_NODE_SIZE = 3\n",
    "GRAPH_TOPIC_NODE_SIZE = 8\n",
    "\n",
    "GRAPH_PARAGRAPH_NODE_SIZE = 5\n",
    "\n",
    "def get_topic_index(topic_distribution: np.ndarray) -> int:\n",
    "    return int(np.argmax(topic_distribution).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b558dca-3488-4803-9969-43b27a4ad5ca",
   "metadata": {},
   "source": [
    "# Topic Modelling with Questions Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcfe10e-7355-4bd5-99cb-bf3bb09ae418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_index(topic_distribution: np.ndarray) -> int:\n",
    "    return int(np.argmax(topic_distribution).item())\n",
    "\n",
    "\n",
    "def get_topic_modeling_question_graph(dataset_folder: str, dataset_name: str, dataset_extension: str = '.json',\n",
    "                                      lda_no_topics: int = 10, lda_max_no_iterations: int = 10,\n",
    "                                      max_no_vertices: Union[int, None] = None,\n",
    "                                      verbose_console: bool = True, verbose_file: bool = False,\n",
    "                                      output_file: Union[None, typing.TextIO] = None) -> GraphFrame:\n",
    "    dataset_path = (\"\" if dataset_folder == \"\" else dataset_folder + \"/\") + dataset_name + dataset_extension\n",
    "\n",
    "    # TODO:Consider saving the LDA results in a file?\n",
    "    dataset_df = spark.read.json(dataset_path, multiLine=True)\n",
    "    dataset_df = dataset_df.withColumn(\"question_text\", (F.col(\"question\"))\n",
    "                                       .cast(\"string\"))\n",
    "\n",
    "    # Tokenization\n",
    "    tokenizer = Tokenizer(inputCol=\"question_text\", outputCol=\"words\")\n",
    "    tokenized_df = tokenizer.transform(dataset_df)\n",
    "\n",
    "    # Get counts in the question text for each word in the raw_features\n",
    "    vectorizer = CountVectorizer(inputCol=\"words\", outputCol=\"raw_features\")\n",
    "    vectorized_model = vectorizer.fit(tokenized_df)\n",
    "    vectorized_df = vectorized_model.transform(tokenized_df)\n",
    "\n",
    "    idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "    idf_model = idf.fit(vectorized_df)\n",
    "    features_df = idf_model.transform(vectorized_df)\n",
    "\n",
    "    # features_df.show()\n",
    "\n",
    "    features_df = features_df.drop('context', 'question', 'supporting_facts', 'words', 'raw_features')\n",
    "\n",
    "    lda = LDA(k=lda_no_topics, maxIter=lda_max_no_iterations, featuresCol=\"features\")\n",
    "    lda_model = lda.fit(features_df)\n",
    "    topic_distribution_df = lda_model.transform(features_df)\n",
    "\n",
    "    select_topic_for_question_function = F.udf(get_topic_index, IntegerType())\n",
    "    question_topic_index_df = topic_distribution_df.withColumn('topic_index',\n",
    "                                                               select_topic_for_question_function(\n",
    "                                                                   topic_distribution_df['topicDistribution']))\n",
    "\n",
    "    question_topic_index_df = question_topic_index_df.drop('features', 'topicDistribution')\n",
    "    question_topic_index_df.show()\n",
    "\n",
    "    # TODO:Attempt to generate the graph directly from df instead of traversing the df since this\n",
    "    #  should offer a speed up...\n",
    "    vertices = []\n",
    "    vertices_scheme = ['id', 'answer', 'level', 'type', 'question', 'topic']\n",
    "\n",
    "    edges = []\n",
    "    edges_scheme = ['src', 'dst']\n",
    "\n",
    "    topic_ids = dict()\n",
    "    for index in range(lda_no_topics):\n",
    "        topic_id = 't' + str(index)\n",
    "        vertices.append((topic_id, '', '', 'topic', '', index))\n",
    "        topic_ids[index] = topic_id\n",
    "\n",
    "    current_index = 0\n",
    "    for row in question_topic_index_df.collect():\n",
    "        node_id = 'q' + str(current_index)\n",
    "        node_topic = row['topic_index']\n",
    "        vertices.append(\n",
    "            (node_id, row['answer'], row['level'], row['type'], row['question_text'], node_topic))\n",
    "        edges.append((node_id, topic_ids[node_topic]))\n",
    "        current_index += 1\n",
    "\n",
    "        if max_no_vertices is not None and current_index >= max_no_vertices:\n",
    "            break\n",
    "\n",
    "    vertices_df = spark.createDataFrame(vertices, vertices_scheme)\n",
    "    edges_df = spark.createDataFrame(edges, edges_scheme)\n",
    "\n",
    "    # edges = []\n",
    "    # edges_scheme = ['src', 'dst']\n",
    "    # # TODO:Extract a df from the question_topic_index_df in order to make this be faster...\n",
    "    # for row in question_topic_index_df.drop('answer', 'level', 'type', 'question_text').collect():\n",
    "    #     edges.append((str(row['_id']), str(row['topic_index'])))\n",
    "    # edges_df = spark.createDataFrame(edges, edges_scheme)\n",
    "\n",
    "    # For the edges,only add between a topic node and nodes with similar outputs\n",
    "\n",
    "    # Get the edges only between a topic node and a question with that topic\n",
    "    # edges_df = question_topic_index_df.drop('answer', 'level', 'type', 'question_text').withColumnRenamed('topic_index',\n",
    "    #                                                                                                       'dst').withColumn(\n",
    "    #     'src', F.col('_id'))\n",
    "    # edges = []\n",
    "    # edges_scheme = ['src', 'dst']\n",
    "    # # TODO:Extract a df from the question_topic_index_df in order to make this be faster...\n",
    "    # for row in question_topic_index_df.drop('answer', 'level', 'type', 'question_text').collect():\n",
    "    #     edges.append((str(row['_id']), str(row['topic_index'])))\n",
    "    # edges_df = spark.createDataFrame(edges, edges_scheme)\n",
    "    # edges_df = edges_df.withColumn(\"src\", F.col(\"src\").cast(StringType())) \\\n",
    "    #     .withColumn(\"dst\", F.col(\"dst\").cast(StringType()))\n",
    "    #\n",
    "    # edges_df.show()\n",
    "    #\n",
    "    # exit(1)\n",
    "\n",
    "    # Add a node for each topic at hand\n",
    "    # for topic_id in range(lda_no_topics):\n",
    "    #     new_row = spark.createDataFrame([Row(str(topic_id), '', '', 'topic', '', str(topic_id))],\n",
    "    #                                     question_topic_index_df.columns)\n",
    "    #     question_topic_index_df = question_topic_index_df.union(new_row)\n",
    "\n",
    "    # exit(1)\n",
    "\n",
    "    # graph = GraphFrame(question_topic_index_df, edges_df)\n",
    "    graph = GraphFrame(vertices_df, edges_df)\n",
    "\n",
    "    no_questions, no_nodes, no_edges = None, None, None\n",
    "    if verbose_console or verbose_file:\n",
    "        no_questions = question_topic_index_df.count()\n",
    "        no_nodes = graph.vertices.count()\n",
    "        no_edges = graph.edges.count()\n",
    "\n",
    "    if verbose_console:\n",
    "        print(f\"Total no questions: {no_questions}\")\n",
    "        print(f\"No vertices: {no_nodes}\")\n",
    "        print(f\"No edges: {no_edges}\")\n",
    "\n",
    "    if verbose_file:\n",
    "        print(f\"Total no questions: {no_questions}\", file=output_file)\n",
    "        print(f\"No vertices: {no_nodes}\", file=output_file)\n",
    "        print(f\"No edges: {no_edges}\", file=output_file)\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def show_topic_modeling_question_graph(dataset_folder: str, dataset_name: str, dataset_extension: str = '.json',\n",
    "                                       lda_no_topics: int = 10, lda_max_no_iterations: int = 10,\n",
    "                                       max_no_vertices: Union[int, None] = None,\n",
    "                                       verbose_console: bool = True, verbose_file: bool = False):\n",
    "    graph = get_topic_modeling_question_graph(dataset_folder, dataset_name, dataset_extension,\n",
    "                                              lda_no_topics=lda_no_topics, lda_max_no_iterations=lda_max_no_iterations,\n",
    "                                              max_no_vertices=max_no_vertices,\n",
    "                                              verbose_console=verbose_console, verbose_file=verbose_file,\n",
    "                                              output_file=None)\n",
    "    nx_graph = nx.empty_graph()\n",
    "    for row in graph.vertices.collect():\n",
    "        node_type = row['type']\n",
    "        node_id = row['id']\n",
    "        group = row['topic']\n",
    "        title, shape, size = None, None, None\n",
    "\n",
    "        if node_type == 'topic':\n",
    "            size = GRAPH_TOPIC_NODE_SIZE\n",
    "            title = f\"Topic {node_id[1:]}\"\n",
    "            shape = 'circle'\n",
    "        else:\n",
    "            answer, level, question_type, question = row['answer'], row['level'], \\\n",
    "                                                     row['type'], row['question']\n",
    "            title = f'Question:{question}\\nAnswer:{answer}\\nLevel:{level}\\nType:{question_type}'\n",
    "\n",
    "            size = GRAPH_QUESTION_NODE_SIZE\n",
    "\n",
    "            shape = 'square'\n",
    "\n",
    "        nx_graph.add_node(node_id, size=size, title=title, group=group, shape=shape)\n",
    "\n",
    "    for edge in graph.edges.collect():\n",
    "        node1 = edge['src']\n",
    "        node2 = edge['dst']\n",
    "        nx_graph.add_edge(node1, node2)\n",
    "\n",
    "    nt = Network('900px', '1700px', select_menu=True, cdn_resources='remote')\n",
    "    nt.from_nx(nx_graph)\n",
    "\n",
    "    #  TODO:Make this also work with the physics and UI menu\n",
    "    html = nt.generate_html()\n",
    "    with open(OUTPUT_FILE_FOLDER + \"/\" + TOPIC_MODELING_QUESTION_GRAPH_NAME + \" Visualization.html\", mode='w',\n",
    "              encoding='utf-8') as html_file:\n",
    "        html_file.write(html)\n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2ec658-bf37-49e5-9fd1-8fd083e0a334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    dataset_folder = 'datasets'\n",
    "    dataset_name = 'hotpot_dev_distractor_v1'\n",
    "\n",
    "    lda_no_topics = 20\n",
    "    lda_max_no_iterations = 10\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    show_topic_modeling_question_graph(dataset_folder, dataset_name,\n",
    "                                       lda_no_topics=lda_no_topics, lda_max_no_iterations=lda_max_no_iterations,\n",
    "                                       max_no_vertices=500)\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    print(f\"Execution time: {execution_time} s\")\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8672dc26-6c83-4d3c-82c8-01a51914cd6a",
   "metadata": {},
   "source": [
    "# Topic Modelling with Questions and Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83860477-4992-4a6c-bb51-fea346a8ec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_modeling_QP_graph(dataset_folder: str, dataset_name: str, dataset_extension: str = '.json',\n",
    "                                lda_no_topics: int = 10, lda_max_no_iterations: int = 10,\n",
    "                                max_no_vertices: Union[int, None] = None,\n",
    "                                verbose_console: bool = True, verbose_file: bool = False,\n",
    "                                output_file: Union[None, typing.TextIO] = None) -> GraphFrame:\n",
    "    # dataset_path = (\"\" if dataset_folder == \"\" else dataset_folder + \"/\") + dataset_name + dataset_extension\n",
    "\n",
    "    # TODO:Consider saving the LDA results in a file?\n",
    "    # TODO:Is this better to use for RaaS than normal reading??\n",
    "\n",
    "    dataset = read_dataset(dataset_folder, dataset_name, dataset_extension)\n",
    "\n",
    "    # questions = []\n",
    "    # questions_scheme = ['id', 'question', 'answer', 'level', 'type']\n",
    "    #\n",
    "    # paragraphs = []\n",
    "    # paragraphs_scheme = ['id', 'title', 'paragraph', 'is_supporting', 'question_id']\n",
    "    # TODO:Better way to represent this?Having 2 dfs has problems with CountVectorizer...\n",
    "    lda_data = []\n",
    "    lda_data_scheme = ['id', 'text']\n",
    "\n",
    "    # Assumptions:Since for paragraphs we don't show/save them,only use them for LDA,there is\n",
    "    # no need to save them in the second field as 'text',thus the second field is just for the question\n",
    "    question_paragraph = []\n",
    "    question_paragraph_scheme = ['id', 'question', 'answer', 'level', 'type', 'title', 'is_supporting',\n",
    "                                 'question_id']\n",
    "    paragraph_ids = dict()\n",
    "    for index, question in enumerate(dataset):\n",
    "        question_id = 'q' + str(index)\n",
    "\n",
    "        # Add info to df for paragraph\n",
    "        supporting_paragraphs = set([supporting_fact[0] for supporting_fact in question['supporting_facts']])\n",
    "        for (title, paragraph) in question['context']:\n",
    "            is_supporting_paragraph = title in supporting_paragraphs\n",
    "            paragraph_text = ''.join(paragraph)\n",
    "\n",
    "            if title not in paragraph_ids:\n",
    "                paragraph_ids[title] = 'p' + str(len(paragraph_ids))\n",
    "            paragraph_id = paragraph_ids[title]\n",
    "\n",
    "            question_paragraph.append(\n",
    "                (paragraph_id, '', '', '', '', title, is_supporting_paragraph, question_id))\n",
    "\n",
    "            lda_data.append((paragraph_id, paragraph_text))\n",
    "\n",
    "        # Add info relevant to question\n",
    "        question_text = question['question']\n",
    "        question_answer, question_level, question_type = question['answer'], question['level'], question['type']\n",
    "        question_paragraph.append(\n",
    "            (question_id, question_text, question_answer, question_level, question_type, '', '', ''))\n",
    "\n",
    "        lda_data.append((question_id, question_text))\n",
    "\n",
    "        if max_no_vertices is not None and len(question_paragraph) >= max_no_vertices:\n",
    "            break\n",
    "\n",
    "    # Strictly info necessary for LDA\n",
    "    lda_question_paragraph_df = spark.createDataFrame(lda_data, lda_data_scheme)\n",
    "\n",
    "    # The rest of the info\n",
    "    question_paragraph_df = spark.createDataFrame(question_paragraph, question_paragraph_scheme)\n",
    "\n",
    "    # question_paragraph_df.show(truncate=False)\n",
    "\n",
    "    # TODO:Consider creating 2 dfs,one for paragraphs and another for questions,and then adding Topic Modeling\n",
    "    #  This should be more eficient,but is also harder to implement than it should be :(\n",
    "    # dataset_df = spark.read.json(dataset_path, multiLine=True)\n",
    "    #\n",
    "    # df1 = dataset_df.withColumn('title-paragraph', F.explode(F.col('context')))\n",
    "    # df2 = df1.withColumn('paragraph', F.col('title-paragraph').getItem(1))\n",
    "    # df3 = df2.withColumn('paragraph2', (F.col(\"paragraph\")).cast('string'))\n",
    "    # df3.drop('context', 'title-paragraph', 'paragraph', 'text').show(truncate=False)\n",
    "\n",
    "    # Tokenization\n",
    "    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "    tokenized_paragraph_df = tokenizer.transform(lda_question_paragraph_df).drop('text')\n",
    "\n",
    "    #\n",
    "    # # Get counts in the question text for each word in the raw_features\n",
    "    vectorizer = CountVectorizer(inputCol=\"tokens\", outputCol=\"raw_features\")\n",
    "    vectorized_model = vectorizer.fit(tokenized_paragraph_df)\n",
    "    vectorized_df = vectorized_model.transform(tokenized_paragraph_df).drop('tokens')\n",
    "\n",
    "    idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "    idf_model = idf.fit(vectorized_df)\n",
    "    features_df = idf_model.transform(vectorized_df).drop('raw_features')\n",
    "\n",
    "    # features_df.show()\n",
    "\n",
    "    lda = LDA(k=lda_no_topics, maxIter=lda_max_no_iterations, featuresCol=\"features\")\n",
    "    lda_model = lda.fit(features_df)\n",
    "    topic_distribution_df = lda_model.transform(features_df).drop('features')\n",
    "\n",
    "    select_topic_for_question_function = F.udf(get_topic_index, IntegerType())\n",
    "    topic_association_df = topic_distribution_df.withColumn('topic',\n",
    "                                                            select_topic_for_question_function(\n",
    "                                                                topic_distribution_df['topicDistribution'])).drop(\n",
    "        'topicDistribution')\n",
    "\n",
    "    qp_topics_df = question_paragraph_df.join(topic_association_df, 'id')\n",
    "\n",
    "    qp_topics_df.show(truncate=False)\n",
    "\n",
    "    # vertices = []\n",
    "    # vertices_scheme = ['id', 'text', 'answer', 'level', 'type', 'title', 'topic']\n",
    "    #\n",
    "    # edges = []\n",
    "    # edges_scheme = ['src', 'dst', 'supporting']\n",
    "    #\n",
    "    # current_index = 0\n",
    "    # for row in qp_topics_df.collect():\n",
    "    #     node_id = row['id']\n",
    "    #     vertices.append(\n",
    "    #         (node_id, row['text'], row['answer'], row['level'], row['type'], row['title'], row['topic']))\n",
    "    #     if 'p' in node_id:\n",
    "    #         edges.append((node_id, row['question_id'], row['is_supporting']))\n",
    "\n",
    "    # vertices_df = spark.createDataFrame(vertices, vertices_scheme)\n",
    "    # edges_df = spark.createDataFrame(edges, edges_scheme)\n",
    "\n",
    "    vertices_df = qp_topics_df.drop('question_id', 'is_supporting')\n",
    "    edges_df = qp_topics_df.drop('question', 'answer', 'level', 'type', 'title', 'topic'). \\\n",
    "        filter(F.col('id').startswith('p')). \\\n",
    "        withColumnRenamed('question_id', 'dst'). \\\n",
    "        withColumnRenamed('id', 'src')\n",
    "\n",
    "    graph = GraphFrame(vertices_df, edges_df)\n",
    "\n",
    "    no_nodes, no_edges = None, None\n",
    "    if verbose_console or verbose_file:\n",
    "        no_nodes = graph.vertices.count()\n",
    "        no_edges = graph.edges.count()\n",
    "\n",
    "    if verbose_console:\n",
    "        print(f\"Total no questions: {len(dataset)}\")\n",
    "        print(f\"No vertices: {no_nodes}\")\n",
    "        print(f\"No edges: {no_edges}\")\n",
    "\n",
    "    if verbose_file:\n",
    "        print(f\"Total no questions: {len(dataset)}\", file=output_file)\n",
    "        print(f\"No vertices: {no_nodes}\", file=output_file)\n",
    "        print(f\"No edges: {no_edges}\", file=output_file)\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def show_topic_modeling_QP_graph(dataset_folder: str, dataset_name: str, dataset_extension: str = '.json',\n",
    "                                 lda_no_topics: int = 10, lda_max_no_iterations: int = 10,\n",
    "                                 max_no_vertices: Union[int, None] = None,\n",
    "                                 verbose_console: bool = True, verbose_file: bool = False):\n",
    "    graph = get_topic_modeling_QP_graph(dataset_folder, dataset_name, dataset_extension,\n",
    "                                        lda_no_topics=lda_no_topics, lda_max_no_iterations=lda_max_no_iterations,\n",
    "                                        max_no_vertices=max_no_vertices,\n",
    "                                        verbose_console=verbose_console, verbose_file=verbose_file,\n",
    "                                        output_file=None)\n",
    "    nx_graph = nx.empty_graph()\n",
    "    for row in graph.vertices.collect():\n",
    "        node_id = row['id']\n",
    "        node_type = node_id[:1]\n",
    "        group = row['topic']\n",
    "        title, shape, size = None, None, None\n",
    "\n",
    "        if node_type == 'p':\n",
    "            size = GRAPH_PARAGRAPH_NODE_SIZE\n",
    "            title = f\"{row['title']}\"\n",
    "            shape = 'circle'\n",
    "        elif node_type == 'q':\n",
    "            answer, level, question_type, question = row['answer'], row['level'], \\\n",
    "                                                     row['type'], row['question']\n",
    "            title = f'Question:{question}\\nAnswer:{answer}\\nLevel:{level}\\nType:{question_type}'\n",
    "\n",
    "            size = GRAPH_QUESTION_NODE_SIZE\n",
    "\n",
    "            shape = 'square'\n",
    "\n",
    "        nx_graph.add_node(node_id, size=size, title=title, group=group, shape=shape)\n",
    "\n",
    "    for edge in graph.edges.collect():\n",
    "        node1 = edge['src']\n",
    "        node2 = edge['dst']\n",
    "        title = ('Is supporting' if edge['is_supporting'] == 'true' else '')\n",
    "\n",
    "        # edge_thickness = (\n",
    "        #     EDGE_IS_SUPPORTING_THICKNESS if edge['is_supporting'] == 'true' else EDGE_IS_NOT_SUPPORTING_THICKNESS)\n",
    "        # value = edge_thickness,\n",
    "        # TODO:Refactor this in a better way\n",
    "        if edge['is_supporting'] == 'true':\n",
    "            nx_graph.add_edge(node1, node2, title=title)\n",
    "        else:\n",
    "            nx_graph.add_edge(node1, node2)\n",
    "\n",
    "    nt = Network('900px', '1700px', select_menu=True, cdn_resources='remote')\n",
    "    nt.from_nx(nx_graph)\n",
    "\n",
    "    #  TODO:Make this also work with the physics and UI menu\n",
    "    html = nt.generate_html()\n",
    "    with open(OUTPUT_FILE_FOLDER + \"/\" + TOPIC_MODELING_QUESTION_PARAGRAPH_GRAPH_NAME + \" Visualization.html\", mode='w',\n",
    "              encoding='utf-8') as html_file:\n",
    "        html_file.write(html)\n",
    "    display(HTML(html))\n",
    "\n",
    "\n",
    "def show_topic_modeling_QP_graph_df(dataset_folder: str, dataset_name: str, dataset_extension: str = '.json',\n",
    "                                    lda_no_topics: int = 10, lda_max_no_iterations: int = 10,\n",
    "                                    max_no_vertices: Union[int, None] = None,\n",
    "                                    verbose_console: bool = True, verbose_file: bool = False):\n",
    "    # Do nothing ,since we don't return anything\n",
    "    if not verbose_console and not verbose_file:\n",
    "        return\n",
    "\n",
    "    output_file = None\n",
    "    if verbose_file:\n",
    "        output_file = open(\n",
    "            OUTPUT_FILE_FOLDER + \"/\" + dataset_name + ' ' + TOPIC_MODELING_QUESTION_PARAGRAPH_GRAPH_NAME + \".txt\", 'w',\n",
    "            encoding=\"utf-8\")\n",
    "\n",
    "    graph = get_topic_modeling_QP_graph(dataset_folder, dataset_name, dataset_extension,\n",
    "                                        lda_no_topics=lda_no_topics, lda_max_no_iterations=lda_max_no_iterations,\n",
    "                                        max_no_vertices=max_no_vertices,\n",
    "                                        verbose_console=verbose_console, verbose_file=verbose_file,\n",
    "                                        output_file=None)\n",
    "\n",
    "    if verbose_console:\n",
    "        print(\"Finding 2 paragraphs that are \")\n",
    "    if verbose_file:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a6fba1-90ce-4565-b102-1265f7fe34b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    dataset_folder = 'datasets'\n",
    "    dataset_name = 'hotpot_dev_distractor_v1'\n",
    "\n",
    "    lda_no_topics = 20\n",
    "    lda_max_no_iterations = 10\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # show_topic_modeling_question_graph(dataset_folder, dataset_name,\n",
    "    #                                    lda_no_topics=lda_no_topics, lda_max_no_iterations=lda_max_no_iterations,\n",
    "    #                                    max_no_vertices=500)\n",
    "\n",
    "    show_topic_modeling_QP_graph(dataset_folder, dataset_name,\n",
    "                                 lda_no_topics=lda_no_topics, lda_max_no_iterations=lda_max_no_iterations,\n",
    "                                 max_no_vertices=25)\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    print(f\"Execution time: {execution_time} s\")\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
