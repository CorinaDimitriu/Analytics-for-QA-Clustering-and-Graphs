{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a10fae0-b964-4b82-b9d8-ab6d1c1b5d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-b051b4dc-749d-4c5d-910b-4ece7bf4789f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.4-spark3.5-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 106ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.4-spark3.5-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-b051b4dc-749d-4c5d-910b-4ece7bf4789f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/4ms)\n",
      "25/01/15 14:30:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/15 14:31:17 WARN TaskSetManager: Stage 2 contains a task of very large size (1418 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no questions: 90447\n",
      "No vertices: 196017\n",
      "No edges: 180894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/15 14:31:21 WARN TaskSetManager: Stage 12 contains a task of very large size (1418 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/01/15 14:31:22 WARN TaskSetManager: Stage 13 contains a task of very large size (1418 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Code for processing files,such as getting the ids of questions that have a common support paragraph,used for clusterization\n",
    "\n",
    "import codecs\n",
    "import json\n",
    "import time\n",
    "import typing\n",
    "from typing import Union\n",
    "\n",
    "import os\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"HotpotQA Clustering\")\n",
    "    .config('spark.executor.instances','2')\n",
    "    .config('spark.executor.memory','4G')\n",
    "    .config(\"spark.driver.memory\", \"4G\")\n",
    "    .config('spark.executor.cores','6')\n",
    "    .config('spark.dynamicAllocation.enabled','false') # musai??\n",
    "    .master('spark://master:7077')\n",
    "    .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.4-spark3.5-s_2.12\")\n",
    "    .getOrCreate())\n",
    "from graphframes import GraphFrame\n",
    "\n",
    "OUTPUT_FILE_FOLDER = 'processed'\n",
    "DATASETS_FOLDER='/home/ubuntu/data/'\n",
    "\n",
    "QUESTION_PARAGRAPH_GRAPH_NAME = 'QP Graph Q1 Q2 P'\n",
    "\n",
    "\n",
    "def read_dataset_with_pyspark(dataset_folder: str, dataset_name: str, dataset_extension: str = \".parquet\"):\n",
    "    dataset_path = dataset_folder + dataset_name\n",
    "\n",
    "    return spark.read.parquet(dataset_path, multiLine=True)\n",
    "\n",
    "\n",
    "def get_question_paragraph_graph(dataset_folder: str, dataset_name: str,\n",
    "                                 max_no_nodes: Union[int, None] = None,\n",
    "                                 verbose_console: bool = True) -> GraphFrame:\n",
    "    # dataset = read_dataset(dataset_folder, dataset_name)\n",
    "    dataset = read_dataset_with_pyspark(dataset_folder, dataset_name)\n",
    "\n",
    "    vertices = []\n",
    "    vertices_scheme = ['id', 'node_type', 'node_text', 'question_answer', 'question_level', 'question_type',\n",
    "                       'dataset_id']\n",
    "    edges = []\n",
    "    edges_scheme = [\"src\", \"dst\"]\n",
    "\n",
    "    # TODO:The number of nodes is suspiciously no of questions * 3...Which implies that there aren't any 2 paragraphs with the same title...\n",
    "    #  Probably not true,meaning the GraphFrame doesn't eliminate duplicate nodes with the same id...Fix\n",
    "\n",
    "    all_paragraph_titles = dict()\n",
    "\n",
    "    for index, question in enumerate(dataset.collect()):\n",
    "        question_node_id = 'q' + str(index)\n",
    "\n",
    "        if max_no_nodes is not None and index >= max_no_nodes:\n",
    "            break\n",
    "\n",
    "        supporting_titles = set()\n",
    "        for supporting_fact in question['supporting_facts']:\n",
    "            supporting_title = supporting_fact[0]\n",
    "            supporting_titles.add(supporting_title)\n",
    "\n",
    "        vertices.append(\n",
    "            (question_node_id, 'q', question['question'], question['answer'], question['level'], question['type'],\n",
    "             question['_id']))\n",
    "\n",
    "        for supporting_title in supporting_titles:\n",
    "            add_paragraph = False\n",
    "            if supporting_title not in all_paragraph_titles:\n",
    "                # in this\n",
    "                all_paragraph_titles[supporting_title] = len(all_paragraph_titles)\n",
    "                add_paragraph = True\n",
    "\n",
    "            title_node_id = 'p' + str(all_paragraph_titles[supporting_title])\n",
    "\n",
    "            if add_paragraph:\n",
    "                vertices.append((title_node_id, 'p', supporting_title, None, None, None, None))\n",
    "\n",
    "            # The question requires the information in this context/paragraph\n",
    "            edges.append((question_node_id, title_node_id))\n",
    "\n",
    "    # TODO:Better refactoring?\n",
    "\n",
    "    vertices = spark.createDataFrame(vertices, vertices_scheme)\n",
    "    edges = spark.createDataFrame(edges, edges_scheme)\n",
    "    graph = GraphFrame(vertices, edges)\n",
    "\n",
    "    no_nodes, no_edges, no_instances = None, None, None\n",
    "    if verbose_console:\n",
    "        no_nodes = graph.vertices.count()\n",
    "        no_edges = graph.edges.count()\n",
    "        no_instances = dataset.count()\n",
    "\n",
    "    if verbose_console:\n",
    "        print(f\"Total no questions: {no_instances}\")\n",
    "        print(f\"No vertices: {no_nodes}\")\n",
    "        print(f\"No edges: {no_edges}\")\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def show_question_paragraph_graph_df(dataset_folder: str, dataset_name: str):\n",
    "    \"\"\"\n",
    "    3.Direct connection by using the titles of the supporting contexts as an edge between questions\n",
    "    \"\"\"\n",
    "\n",
    "    graph = get_question_paragraph_graph(dataset_folder, dataset_name,\n",
    "                                         max_no_nodes=None)\n",
    "\n",
    "    df_questions_same_relevant_paragraph = graph.find(\"(q1)-[]->(p);(q2)-[]->(p)\"). \\\n",
    "        filter(\"q1.node_type='q'\").filter(\"q2.node_type='q'\"). \\\n",
    "        filter(\"p.node_type='p'\"). \\\n",
    "        filter(\"q1.id < q2.id\"). \\\n",
    "        dropDuplicates(['q1', 'q2', 'p'])\n",
    "\n",
    "    info = dict()\n",
    "\n",
    "    for pattern in df_questions_same_relevant_paragraph.collect():\n",
    "        paragraph_title = pattern['p']['node_text']\n",
    "        if paragraph_title not in info:\n",
    "            info[paragraph_title] = set()\n",
    "\n",
    "        info[paragraph_title].add(pattern['q1']['dataset_id'])\n",
    "        info[paragraph_title].add(pattern['q2']['dataset_id'])\n",
    "\n",
    "    serializable_info = dict()\n",
    "    for title, ids in info.items():\n",
    "        serializable_info[title] = list(ids)\n",
    "\n",
    "    with open(OUTPUT_FILE_FOLDER + \"/\" + dataset_name + ' ' + QUESTION_PARAGRAPH_GRAPH_NAME + \".json\", 'w',\n",
    "              encoding=\"utf-8\") as file:\n",
    "        json.dump(serializable_info, file)\n",
    "\n",
    "\n",
    "def main():\n",
    "    dataset_folder = DATASETS_FOLDER\n",
    "    dataset_name = 'hotpot_train_v1.1'\n",
    "\n",
    "    show_question_paragraph_graph_df(dataset_folder, dataset_name)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bf81a3-4004-4f42-8274-6224593f695c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
