{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12beaf0c-9e49-4082-85a4-20ffef278c77",
   "metadata": {},
   "source": [
    "Create NER Graph.COde for saving and loading from an already processed graph is also included for better efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a976dc8-84e2-4710-b5e8-6720d95428ee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Load spark and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21c488e9-0708-423f-a9a2-831915d3a1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ac919849-b387-4a67-b342-88bac9a3bbeb;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.4-spark3.5-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;5.5.2 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-s3;1.12.500 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-kms;1.12.500 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-core;1.12.500 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound software.amazon.ion#ion-java;1.0.2 in central\n",
      "\tfound joda-time#joda-time;2.8.1 in central\n",
      "\tfound com.amazonaws#jmespath-java;1.12.500 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound com.google.cloud#google-cloud-storage;2.20.1 in central\n",
      "\tfound com.google.guava#guava;31.1-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.18.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound com.google.http-client#google-http-client;1.43.0 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in central\n",
      "\tfound com.google.http-client#google-http-client-jackson2;1.43.0 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.43.0 in central\n",
      "\tfound com.google.api-client#google-api-client;2.2.0 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.34.1 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.43.0 in central\n",
      "\tfound com.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 in central\n",
      "\tfound com.google.code.gson#gson;2.10.1 in central\n",
      "\tfound com.google.cloud#google-cloud-core;2.12.0 in central\n",
      "\tfound io.grpc#grpc-context;1.53.0 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.10.1 in central\n",
      "\tfound com.google.auto.value#auto-value;1.10.1 in central\n",
      "\tfound javax.annotation#javax.annotation-api;1.3.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-http;2.12.0 in central\n",
      "\tfound com.google.http-client#google-http-client-appengine;1.43.0 in central\n",
      "\tfound com.google.api#gax-httpjson;0.108.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-grpc;2.12.0 in central\n",
      "\tfound io.grpc#grpc-alts;1.53.0 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.53.0 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n",
      "\tfound io.grpc#grpc-auth;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.53.0 in central\n",
      "\tfound io.grpc#grpc-core;1.53.0 in central\n",
      "\tfound com.google.api#gax;2.23.2 in central\n",
      "\tfound com.google.api#gax-grpc;2.23.2 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;1.16.0 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;1.16.0 in central\n",
      "\tfound com.google.api#api-common;2.6.2 in central\n",
      "\tfound io.opencensus#opencensus-api;0.31.1 in central\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.9.2 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.12 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.21.12 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.14.2 in central\n",
      "\tfound org.threeten#threetenbp;1.6.5 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound io.grpc#grpc-api;1.53.0 in central\n",
      "\tfound io.grpc#grpc-stub;1.53.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.31.0 in central\n",
      "\tfound io.perfmark#perfmark-api;0.26.0 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.22 in central\n",
      "\tfound io.opencensus#opencensus-proto;0.2.0 in central\n",
      "\tfound io.grpc#grpc-services;1.53.0 in central\n",
      "\tfound com.google.re2j#re2j;1.6 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.53.0 in central\n",
      "\tfound io.grpc#grpc-googleapis;1.53.0 in central\n",
      "\tfound io.grpc#grpc-xds;1.53.0 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound org.jsoup#jsoup;1.18.2 in central\n",
      "\tfound jakarta.mail#jakarta.mail-api;2.1.3 in central\n",
      "\tfound jakarta.activation#jakarta.activation-api;2.1.3 in central\n",
      "\tfound org.eclipse.angus#angus-mail;2.0.3 in central\n",
      "\tfound org.eclipse.angus#angus-activation;2.0.2 in central\n",
      "\tfound org.apache.poi#poi-ooxml;4.1.2 in central\n",
      "\tfound org.apache.poi#poi;4.1.2 in central\n",
      "\tfound org.apache.commons#commons-collections4;4.4 in central\n",
      "\tfound org.apache.commons#commons-math3;3.6.1 in central\n",
      "\tfound com.zaxxer#SparseBitSet;1.2 in central\n",
      "\tfound org.apache.poi#poi-ooxml-schemas;4.1.2 in central\n",
      "\tfound org.apache.xmlbeans#xmlbeans;3.1.0 in central\n",
      "\tfound org.apache.commons#commons-compress;1.19 in central\n",
      "\tfound com.github.virtuald#curvesapi;1.06 in central\n",
      "\tfound org.apache.poi#poi-scratchpad;4.1.2 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 in central\n",
      "\tfound com.microsoft.onnxruntime#onnxruntime;1.19.2 in central\n",
      "\tfound com.johnsnowlabs.nlp#jsl-llamacpp-cpu_2.12;0.1.4 in central\n",
      "\tfound org.jetbrains#annotations;24.1.0 in central\n",
      "\tfound com.johnsnowlabs.nlp#jsl-openvino-cpu_2.12;0.1.0 in central\n",
      ":: resolution report :: resolve 1592ms :: artifacts dl 69ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-core;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-kms;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-s3;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#jmespath-java;1.12.500 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.github.virtuald#curvesapi;1.06 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api#api-common;2.6.2 from central in [default]\n",
      "\tcom.google.api#gax;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-grpc;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-httpjson;0.108.2 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;2.2.0 from central in [default]\n",
      "\tcom.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.14.2 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.9.2 from central in [default]\n",
      "\tcom.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;1.16.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;1.16.0 from central in [default]\n",
      "\tcom.google.auto.value#auto-value;1.10.1 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.10.1 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-grpc;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-http;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-storage;2.20.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.10.1 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.18.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;31.1-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.http-client#google-http-client;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-appengine;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-jackson2;1.43.0 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.34.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.21.12 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.21.12 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.6 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#jsl-llamacpp-cpu_2.12;0.1.4 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#jsl-openvino-cpu_2.12;0.1.0 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;5.5.2 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 from central in [default]\n",
      "\tcom.microsoft.onnxruntime#onnxruntime;1.19.2 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tcom.zaxxer#SparseBitSet;1.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tgraphframes#graphframes;0.8.4-spark3.5-s_2.12 from spark-packages in [default]\n",
      "\tio.grpc#grpc-alts;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-api;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-context;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-core;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-googleapis;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-services;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-xds;1.53.0 from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-proto;0.2.0 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.26.0 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjakarta.activation#jakarta.activation-api;2.1.3 from central in [default]\n",
      "\tjakarta.mail#jakarta.mail-api;2.1.3 from central in [default]\n",
      "\tjavax.annotation#javax.annotation-api;1.3.2 from central in [default]\n",
      "\tjoda-time#joda-time;2.8.1 from central in [default]\n",
      "\torg.apache.commons#commons-collections4;4.4 from central in [default]\n",
      "\torg.apache.commons#commons-compress;1.19 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.6.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.apache.poi#poi;4.1.2 from central in [default]\n",
      "\torg.apache.poi#poi-ooxml;4.1.2 from central in [default]\n",
      "\torg.apache.poi#poi-ooxml-schemas;4.1.2 from central in [default]\n",
      "\torg.apache.poi#poi-scratchpad;4.1.2 from central in [default]\n",
      "\torg.apache.xmlbeans#xmlbeans;3.1.0 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.31.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.22 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n",
      "\torg.eclipse.angus#angus-activation;2.0.2 from central in [default]\n",
      "\torg.eclipse.angus#angus-mail;2.0.3 from central in [default]\n",
      "\torg.jetbrains#annotations;24.1.0 from central in [default]\n",
      "\torg.jsoup#jsoup;1.18.2 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\torg.threeten#threetenbp;1.6.5 from central in [default]\n",
      "\tsoftware.amazon.ion#ion-java;1.0.2 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcommons-logging#commons-logging;1.2 by [commons-logging#commons-logging;1.1.3] in [default]\n",
      "\tcommons-codec#commons-codec;1.11 by [commons-codec#commons-codec;1.15] in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 by [com.google.protobuf#protobuf-java-util;3.21.12] in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 by [com.google.protobuf#protobuf-java;3.21.12] in [default]\n",
      "\tcom.google.code.gson#gson;2.3 by [com.google.code.gson#gson;2.10.1] in [default]\n",
      "\tcommons-codec#commons-codec;1.13 by [commons-codec#commons-codec;1.15] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |  104  |   0   |   0   |   6   ||   98  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ac919849-b387-4a67-b342-88bac9a3bbeb\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 98 already retrieved (0kB/31ms)\n",
      "25/01/16 22:49:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sparknlp.base import DocumentAssembler, Pipeline\n",
    "from sparknlp.annotator import (\n",
    "    Tokenizer,\n",
    "    WordEmbeddingsModel,\n",
    "    NerDLModel,\n",
    "    NerConverter, BertEmbeddings\n",
    ")\n",
    "\n",
    "import time\n",
    "import typing\n",
    "from typing import Union\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyvis.network import Network\n",
    "import networkx as nx\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import os\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "StopWords = stopwords.words(\"english\")\n",
    "no_groups = 10\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"HotpotQA Clustering\")\n",
    "    .config('spark.executor.instances','2')\n",
    "    .config('spark.executor.memory','12G')\n",
    "    .config(\"spark.driver.memory\", \"4G\")\n",
    "    .config('spark.executor.cores','4')\n",
    "    .config('spark.dynamicAllocation.enabled','false') # musai??\n",
    "    .master('spark://master:7077')\n",
    "    .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.4-spark3.5-s_2.12,com.johnsnowlabs.nlp:spark-nlp_2.12:5.5.2\")\n",
    "    .getOrCreate()\n",
    "        )\n",
    "\n",
    "  # .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.4-spark3.5-s_2.12\")\n",
    "\n",
    "from graphframes import GraphFrame\n",
    "\n",
    "\n",
    "def load_graph(full_graph_folder: str,\n",
    "                                 full_graph_name: str):\n",
    "    vertices_path = full_graph_folder + '/' + full_graph_name + ' Vertices'\n",
    "    edges_path = full_graph_folder + '/' + full_graph_name + ' Edges'\n",
    "\n",
    "    vertices = spark.read.parquet(vertices_path)\n",
    "    edges = spark.read.parquet(edges_path)\n",
    "\n",
    "    return GraphFrame(vertices, edges)\n",
    "    \n",
    "\n",
    "# # Load the HotpotQA JSON Data\n",
    "# dataset_name='hotpot_dev_distractor_v1'\n",
    "# dataset_extension='.json'\n",
    "# dataset_path='/home/ubuntu/data/' + dataset_name + dataset_extension\n",
    "# df = spark.read.json(dataset_path, multiLine=True).withColumn(\"text_q\",\n",
    "#                                                               col(\"question\")\n",
    "#                                                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf167c65-6b87-466d-8234-ab80688fe707",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Create pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6f2cf9-041f-4431-8c55-350bb98ac3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, DoubleType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text_q\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"tokens\")\n",
    "\n",
    "# Get the embeddings using glove_100d\n",
    "embeddings = (WordEmbeddingsModel.pretrained('glove_100d', 'en')\n",
    "              .setInputCols([\"document\", 'tokens'])) \\\n",
    "              .setOutputCol(\"embeddings\")\n",
    "# embeddings = BertEmbeddings.pretrained('bert_base_uncased', 'en') \\\n",
    "#     .setInputCols([\"document\", 'token']) \\\n",
    "#     .setOutputCol(\"embeddings\")\n",
    "\n",
    "public_ner = NerDLModel.pretrained('ner_dl', 'en') \\\n",
    "    .setInputCols([\"document\", \"tokens\", \"embeddings\"]) \\\n",
    "    .setOutputCol(\"ner\")\n",
    "\n",
    "ner_converter = NerConverter() \\\n",
    "    .setInputCols([\"document\", \"tokens\", \"ner\"]) \\\n",
    "    .setOutputCol(\"entities\")\n",
    "\n",
    "# Define and fit the pipeline\n",
    "nlpPipeline = Pipeline(stages=[documentAssembler,\n",
    "                               tokenizer,\n",
    "                               embeddings,\n",
    "                               public_ner,\n",
    "                               ner_converter\n",
    "                               ])\n",
    "pipelineModel = nlpPipeline.fit(df)\n",
    "ner_distribution_df = pipelineModel.transform(df)\n",
    "# result_with_entities = ner_distribution_df.select(\"text_q\", \"entities.result\").rdd.map(\n",
    "#     lambda row: (row['text_q'], row['result'])\n",
    "# ).collect()\n",
    "# questions = [entry[0] for entry in result_with_entities]\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "from pyspark.sql.functions import col, expr\n",
    "ner_distribution_df.printSchema()\n",
    "# Extract relevant columns\n",
    "tokens_and_embeddings_df = ner_distribution_df.select(\n",
    "    \"question\",\n",
    "    \"context\",\n",
    "    \"level\",\n",
    "    \"type\",\n",
    "    'supporting_facts',\n",
    "    \"entities.result\",    # List of entities\n",
    "    col(\"entities.begin\").alias(\"entities_begin\"),   # Entity begin indices\n",
    "    col(\"entities.end\").alias(\"entities_end\"),       # Entity end indices\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed0945f-e9ae-408a-8ae4-bc998fa503ed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Info about extracted info using pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b445092d-e3fa-41a5-84f6-96ae8eb201ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['question', 'context', 'level', 'type', 'supporting_facts', 'result', 'entities_begin', 'entities_end']\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "print(tokens_and_embeddings_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3090a593-1e6e-476a-8b5f-afa49aa0caf9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Get supporting titles(for keeping only supporting paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0658314-ee57-4cf4-971a-406f27024e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['question', 'q_entities', 'level', 'type', 'row_id', 'paragraph_id', 'paragraph', 'title']\n"
     ]
    }
   ],
   "source": [
    "# tokens_and_embeddings_df.select('supporting_facts').show(5,truncate=False)\n",
    "\n",
    "# For testing/limiting\n",
    "max_no_questions=1000\n",
    "\n",
    "df1=tokens_and_embeddings_df.limit(max_no_questions).withColumn('row_id',F.monotonically_increasing_id()).withColumnRenamed('result','q_entities')\n",
    "\n",
    "paragraphs_df = df1.select(\n",
    "    'question',\n",
    "    'q_entities',\n",
    "    'level',\n",
    "    'type',\n",
    "    'row_id',\n",
    "    F.posexplode(F.col('context')).alias('paragraph_id','title-paragraph')\n",
    ")\n",
    "\n",
    "paragraphs_df = paragraphs_df.withColumn('paragraph', F.col('title-paragraph').getItem(1)).withColumn('title',F.col('title-paragraph').getItem(0)).drop('title-paragraph')\n",
    "\n",
    "df2=df1.select('supporting_facts','row_id')\n",
    "df2=df2.withColumn('supporting_pairs',F.explode(F.col('supporting_facts'))).drop('supporting_facts')\n",
    "supporting_titles_df=df2.withColumn('supporting_title',F.col('supporting_pairs').getItem(0)).drop('supporting_pairs').dropDuplicates(['supporting_title','row_id'])\n",
    "\n",
    "# Drop the paragraphs that are not suportting\n",
    "support_paragraphs=paragraphs_df.join(supporting_titles_df,(supporting_titles_df.supporting_title==paragraphs_df.title)&(supporting_titles_df.row_id==paragraphs_df.row_id),'inner').select(paragraphs_df['*'])\n",
    "\n",
    "print(support_paragraphs.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cd13bc-e237-4329-911d-596c3919be60",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1.Sentence Level Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c379883c-9c78-4c3e-b9f6-fd5fcce9a66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-----------+--------------------------------------------------------------------------------------------------------------+\n",
      "|row_id|paragraph_id|sentence_id|s_entities                                                                                                    |\n",
      "+------+------------+-----------+--------------------------------------------------------------------------------------------------------------+\n",
      "|0     |1           |0          |[Scott Derrickson, American]                                                                                  |\n",
      "|0     |1           |1          |[Los Angeles, California]                                                                                     |\n",
      "|0     |1           |2          |[Emily Rose, Deliver Us, Marvel Cinematic Universe, Doctor Strange]                                           |\n",
      "|0     |4           |0          |[Edward Davis Wood Jr, American]                                                                              |\n",
      "|1     |1           |0          |[Shirley Temple Black, American, Hollywood's]                                                                 |\n",
      "|1     |1           |1          |[United States, Ghana, Czechoslovakia, United States]                                                         |\n",
      "|1     |6           |0          |[Kiss, American, Shirley Temple, Corliss Archer]                                                              |\n",
      "|1     |6           |1          |[]                                                                                                            |\n",
      "|1     |6           |2          |[]                                                                                                            |\n",
      "|2     |2           |0          |[Hork-Bajir Chronicles, Animorphs, Applegate]                                                                 |\n",
      "|2     |2           |1          |[Pretender, Ellimist Chronicles, Andalite Chronicles]                                                         |\n",
      "|2     |2           |2          |[Tobias, Hork-Bajir, Jara Hamee, Yeerks, Hork-Bajir, Aldrea, Andalite, Dak Hamee, Hork-Bajir]                 |\n",
      "|2     |2           |3          |[Jara Hamee's, Aldrea, Dak Hamee, Esplin, Megamorphs]                                                         |\n",
      "|2     |8           |0          |[Katherine Applegate, Michael Grant, K, Applegate, Scholastic]                                                |\n",
      "|2     |8           |1          |[]                                                                                                            |\n",
      "|2     |8           |2          |[]                                                                                                            |\n",
      "|3     |5           |0          |[Laleli Mosque, Turkish, Laleli Camii, Tulip Mosque, Ottoman, Laleli, Fatih, Istanbul, Turkey]                |\n",
      "|3     |6           |0          |[Esma Sultan Mansion, Turkish, Esma Sultan Yalısı, English, Bosphorus, Ortaköy, Istanbul, Turkey, Esma Sultan]|\n",
      "|4     |3           |0          |[Adriana Trigiani, Italian, American, Greenwich Village, New York City]                                       |\n",
      "|4     |3           |1          |[Trigiani]                                                                                                    |\n",
      "+------+------------+-----------+--------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-----------+----------+-------------------------+-------+-----------+\n",
      "|row_id|paragraph_id|sentence_id|entity_pos|entity                   |id     |entity_type|\n",
      "+------+------------+-----------+----------+-------------------------+-------+-----------+\n",
      "|0     |1           |0          |0         |Scott Derrickson         |0-1-0-0|s          |\n",
      "|0     |1           |0          |1         |American                 |0-1-0-1|s          |\n",
      "|0     |1           |1          |0         |Los Angeles              |0-1-1-0|s          |\n",
      "|0     |1           |1          |1         |California               |0-1-1-1|s          |\n",
      "|0     |1           |2          |0         |Emily Rose               |0-1-2-0|s          |\n",
      "|0     |1           |2          |1         |Deliver Us               |0-1-2-1|s          |\n",
      "|0     |1           |2          |2         |Marvel Cinematic Universe|0-1-2-2|s          |\n",
      "|0     |1           |2          |3         |Doctor Strange           |0-1-2-3|s          |\n",
      "|0     |4           |0          |0         |Edward Davis Wood Jr     |0-4-0-0|s          |\n",
      "|0     |4           |0          |1         |American                 |0-4-0-1|s          |\n",
      "|1     |1           |0          |0         |Shirley Temple Black     |1-1-0-0|s          |\n",
      "|1     |1           |0          |1         |American                 |1-1-0-1|s          |\n",
      "|1     |1           |0          |2         |Hollywood's              |1-1-0-2|s          |\n",
      "|1     |1           |1          |0         |United States            |1-1-1-0|s          |\n",
      "|1     |1           |1          |1         |Ghana                    |1-1-1-1|s          |\n",
      "|1     |1           |1          |2         |Czechoslovakia           |1-1-1-2|s          |\n",
      "|1     |1           |1          |3         |United States            |1-1-1-3|s          |\n",
      "|1     |6           |0          |0         |Kiss                     |1-6-0-0|s          |\n",
      "|1     |6           |0          |1         |American                 |1-6-0-1|s          |\n",
      "|1     |6           |0          |2         |Shirley Temple           |1-6-0-2|s          |\n",
      "+------+------------+-----------+----------+-------------------------+-------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------------------+-------+-------+\n",
      "|entity1                  |entity2                  |src    |dst    |\n",
      "+-------------------------+-------------------------+-------+-------+\n",
      "|Scott Derrickson         |American                 |0-1-0-0|0-1-0-1|\n",
      "|American                 |Scott Derrickson         |0-1-0-1|0-1-0-0|\n",
      "|Los Angeles              |California               |0-1-1-0|0-1-1-1|\n",
      "|California               |Los Angeles              |0-1-1-1|0-1-1-0|\n",
      "|Emily Rose               |Deliver Us               |0-1-2-0|0-1-2-1|\n",
      "|Emily Rose               |Marvel Cinematic Universe|0-1-2-0|0-1-2-2|\n",
      "|Emily Rose               |Doctor Strange           |0-1-2-0|0-1-2-3|\n",
      "|Deliver Us               |Emily Rose               |0-1-2-1|0-1-2-0|\n",
      "|Deliver Us               |Marvel Cinematic Universe|0-1-2-1|0-1-2-2|\n",
      "|Deliver Us               |Doctor Strange           |0-1-2-1|0-1-2-3|\n",
      "|Marvel Cinematic Universe|Emily Rose               |0-1-2-2|0-1-2-0|\n",
      "|Marvel Cinematic Universe|Deliver Us               |0-1-2-2|0-1-2-1|\n",
      "|Marvel Cinematic Universe|Doctor Strange           |0-1-2-2|0-1-2-3|\n",
      "|Doctor Strange           |Emily Rose               |0-1-2-3|0-1-2-0|\n",
      "|Doctor Strange           |Deliver Us               |0-1-2-3|0-1-2-1|\n",
      "|Doctor Strange           |Marvel Cinematic Universe|0-1-2-3|0-1-2-2|\n",
      "|Edward Davis Wood Jr     |American                 |0-4-0-0|0-4-0-1|\n",
      "|American                 |Edward Davis Wood Jr     |0-4-0-1|0-4-0-0|\n",
      "|Shirley Temple Black     |American                 |1-1-0-0|1-1-0-1|\n",
      "|Shirley Temple Black     |Hollywood's              |1-1-0-0|1-1-0-2|\n",
      "+-------------------------+-------------------------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# .withColumnRenamed('position','paragraph_id')\n",
    "\n",
    "def get_sentence_entities_df():\n",
    "    df3=support_paragraphs.drop('q_entities','level','type').withColumn('paragraph_json',F.from_json(F.col('paragraph'), ArrayType(StringType()))).drop('paragraph')\n",
    "    # df3=df3.withColumn('sentence',F.explode(F.col('paragraph_json')))\n",
    "    # df3.select('sentence','row_id').show(5,truncate=False)\n",
    "    \n",
    "    df3 = df3.select(\n",
    "        'question',\n",
    "        'row_id',\n",
    "        'paragraph_id',\n",
    "         F.posexplode(F.col('paragraph_json')).alias('sentence_id','sentence')\n",
    "    )\n",
    "    # paragraphs_df = paragraphs_df.withColumn('paragraph', F.col('title-paragraph').getItem(1)).withColumn('title',F.col('title-paragraph').getItem(0)).drop('title-paragraph')\n",
    "    # df3.select('row_id','paragraph_id','sentence_id','sentence').show(truncate=False)\n",
    "    \n",
    "    sentence_entities_df=df3.withColumnRenamed('sentence','text_q')\n",
    "    sentence_entities_df=pipelineModel.transform(sentence_entities_df).withColumnRenamed('text_q','sentence').drop('sentence')\n",
    "\n",
    "    sentence_entities_df=sentence_entities_df.select(\n",
    "        F.col('row_id'),\n",
    "        F.col('paragraph_id'),\n",
    "        F.col('sentence_id'),\n",
    "        F.col('entities.result').alias('s_entities')\n",
    "    ).dropDuplicates(['row_id','paragraph_id','sentence_id'])\n",
    "    \n",
    "    # sentence_entities_df.show(truncate=False\n",
    "\n",
    "    return sentence_entities_df\n",
    "\n",
    "\n",
    "def get_split_sentence_entities_df():\n",
    "    sentence_entities_df=get_sentence_entities_df()\n",
    "    \n",
    "    split_sentence_entities_df=sentence_entities_df.select(\n",
    "        F.col('row_id'),\n",
    "        F.col('paragraph_id'),\n",
    "        F.col('sentence_id'),\n",
    "        F.posexplode(F.col('s_entities')).alias('entity_pos','entity'),\n",
    "    ).withColumn('id',F.concat(F.col(\"row_id\"),F.lit(\"-\"), F.col(\"paragraph_id\"),F.lit(\"-\"),F.col('sentence_id'),F.lit('-'),F.col('entity_pos'))).dropDuplicates(['id'])\\\n",
    "    .withColumn('entity_type',F.lit('s'))\n",
    "\n",
    "    return split_sentence_entities_df\n",
    "\n",
    "\n",
    "def get_sentence_links_df():\n",
    "    split_sentence_entities_df=get_split_sentence_entities_df()\n",
    "    \n",
    "    sentence_links_df = (\n",
    "        split_sentence_entities_df.alias(\"df1\")\n",
    "        .join(\n",
    "            split_sentence_entities_df.alias(\"df2\"),\n",
    "            (F.col(\"df1.row_id\") == F.col(\"df2.row_id\")) &  \n",
    "            (F.col(\"df1.paragraph_id\") == F.col(\"df2.paragraph_id\")) & \n",
    "            (F.col(\"df1.sentence_id\") == F.col(\"df2.sentence_id\")) & \n",
    "            (F.col(\"df1.entity\") != F.col(\"df2.entity\")),\n",
    "            \"inner\"\n",
    "        )\n",
    "        .select(\n",
    "            F.col(\"df1.entity\").alias(\"entity1\"),\n",
    "            F.col(\"df2.entity\").alias(\"entity2\"),\n",
    "            F.col('df1.id').alias('src'),\n",
    "            F.col('df2.id').alias('dst')\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return sentence_links_df\n",
    "\n",
    "\n",
    "get_sentence_entities_df().show(truncate=False)\n",
    "get_split_sentence_entities_df().show(truncate=False)\n",
    "get_sentence_links_df().show(truncate=False)\n",
    "\n",
    "\n",
    "# print(df3.columns)\n",
    "# df3=df3.select('paragraph','row_id').withColumn('paragraph_json',F.from_json(F.col('paragraph'), ArrayType(StringType()))).drop('paragraph')\n",
    "# df3=df3.withColumn('sentence',F.explode(F.col('paragraph_json')))\n",
    "# df3.select('sentence','row_id').show(5,truncate=False)\n",
    "\n",
    "# Drop the paragraphs that are not suportting\n",
    "# df3=paragraphs_df.join(supporting_titles_df,(supporting_titles_df.supporting_title==paragraphs_df.title)&(supporting_titles_df.row_id==paragraphs_df.row_id),'inner').select(paragraphs_df['*'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eef7499-78bb-48bb-8472-d40e0773f41f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.Context Level Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2f40fc5-0e6c-45ba-b4db-40ce7f5b36c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+-------+-------+\n",
      "|entity1         |entity2         |src    |dst    |\n",
      "+----------------+----------------+-------+-------+\n",
      "|American        |American        |0-1-0-1|0-4-0-1|\n",
      "|American        |American        |0-4-0-1|0-1-0-1|\n",
      "|American        |American        |1-1-0-1|1-6-0-1|\n",
      "|American        |American        |1-6-0-1|1-1-0-1|\n",
      "|Applegate       |Applegate       |2-2-0-2|2-8-0-3|\n",
      "|Applegate       |Applegate       |2-8-0-3|2-2-0-2|\n",
      "|Istanbul        |Istanbul        |3-5-0-7|3-6-0-6|\n",
      "|Istanbul        |Istanbul        |3-6-0-6|3-5-0-7|\n",
      "|Turkey          |Turkey          |3-5-0-8|3-6-0-7|\n",
      "|Turkey          |Turkey          |3-6-0-7|3-5-0-8|\n",
      "|Turkish         |Turkish         |3-5-0-1|3-6-0-1|\n",
      "|Turkish         |Turkish         |3-6-0-1|3-5-0-1|\n",
      "|Adriana Trigiani|Adriana Trigiani|4-3-0-0|4-9-0-2|\n",
      "|Adriana Trigiani|Adriana Trigiani|4-9-0-2|4-3-0-0|\n",
      "|American        |American        |4-3-0-2|4-9-0-1|\n",
      "|American        |American        |4-9-0-1|4-3-0-2|\n",
      "+----------------+----------------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_context_level_links_df():\n",
    "    split_sentence_entities_df=get_split_sentence_entities_df()\n",
    "\n",
    "    context_level_links_df=split_sentence_entities_df.alias('df1').join(\n",
    "        split_sentence_entities_df.alias('df2'),\n",
    "        (F.col('df1.row_id')==F.col('df2.row_id')) &\n",
    "        (F.col('df1.paragraph_id')!=F.col('df2.paragraph_id')) &\n",
    "        (F.col('df1.entity')==F.col('df2.entity')),\n",
    "        'inner'\n",
    "    ).select(\n",
    "        F.col(\"df1.entity\").alias(\"entity1\"),\n",
    "        F.col(\"df2.entity\").alias(\"entity2\"),\n",
    "        F.col('df1.id').alias('src'),\n",
    "        F.col('df2.id').alias('dst'),\n",
    "    )\n",
    "\n",
    "    return context_level_links_df\n",
    "    \n",
    "\n",
    "get_context_level_links_df().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c38b62-bc95-4e98-9f2e-c8fa2c0e70c7",
   "metadata": {},
   "source": [
    "## 3.Paragraph Level Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d71b489d-1857-4812-bf3a-a2cfec414b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+------+------------+-----------------------+-----------+\n",
      "|title                    |row_id|paragraph_id|t_entities             |sentence_id|\n",
      "+-------------------------+------+------------+-----------------------+-----------+\n",
      "|Scott Derrickson         |0     |1           |[Scott Derrickson]     |-1         |\n",
      "|Ed Wood                  |0     |4           |[Ed Wood]              |-1         |\n",
      "|Shirley Temple           |1     |1           |[Shirley Temple]       |-1         |\n",
      "|Kiss and Tell (1945 film)|1     |6           |[]                     |-1         |\n",
      "|The Hork-Bajir Chronicles|2     |2           |[Hork-Bajir Chronicles]|-1         |\n",
      "|Animorphs                |2     |8           |[]                     |-1         |\n",
      "|Laleli Mosque            |3     |5           |[Laleli Mosque]        |-1         |\n",
      "|Esma Sultan Mansion      |3     |6           |[Esma Sultan Mansion]  |-1         |\n",
      "|Adriana Trigiani         |4     |3           |[Adriana Trigiani]     |-1         |\n",
      "|Big Stone Gap (film)     |4     |9           |[Big Stone Gap]        |-1         |\n",
      "+-------------------------+------+------------+-----------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-----------+----------+---------------------+--------+-----------+\n",
      "|row_id|paragraph_id|sentence_id|entity_pos|entity               |id      |entity_type|\n",
      "+------+------------+-----------+----------+---------------------+--------+-----------+\n",
      "|0     |1           |-1         |0         |Scott Derrickson     |0-1--1-0|t          |\n",
      "|0     |4           |-1         |0         |Ed Wood              |0-4--1-0|t          |\n",
      "|1     |1           |-1         |0         |Shirley Temple       |1-1--1-0|t          |\n",
      "|2     |2           |-1         |0         |Hork-Bajir Chronicles|2-2--1-0|t          |\n",
      "|3     |5           |-1         |0         |Laleli Mosque        |3-5--1-0|t          |\n",
      "|3     |6           |-1         |0         |Esma Sultan Mansion  |3-6--1-0|t          |\n",
      "|4     |3           |-1         |0         |Adriana Trigiani     |4-3--1-0|t          |\n",
      "|4     |9           |-1         |0         |Big Stone Gap        |4-9--1-0|t          |\n",
      "+------+------------+-----------+----------+---------------------+--------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------+--------+-------+\n",
      "|entity1              |entity2              |src     |dst    |\n",
      "+---------------------+---------------------+--------+-------+\n",
      "|Scott Derrickson     |Scott Derrickson     |0-1--1-0|0-1-0-0|\n",
      "|Hork-Bajir Chronicles|Hork-Bajir Chronicles|2-2--1-0|2-2-0-0|\n",
      "|Laleli Mosque        |Laleli Mosque        |3-5--1-0|3-5-0-0|\n",
      "|Esma Sultan Mansion  |Esma Sultan Mansion  |3-6--1-0|3-6-0-0|\n",
      "|Adriana Trigiani     |Adriana Trigiani     |4-3--1-0|4-3-0-0|\n",
      "|Big Stone Gap        |Big Stone Gap        |4-9--1-0|4-9-0-0|\n",
      "+---------------------+---------------------+--------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# .withColumnRenamed('position','paragraph_id')\n",
    "\n",
    "def get_title_entities_df():\n",
    "    df3=support_paragraphs.drop('q_entities','level','type')\n",
    "    # df3=df3.withColumn('sentence',F.explode(F.col('paragraph_json')))\n",
    "    # df3.select('sentence','row_id').show(5,truncate=False)\n",
    "    \n",
    "    df3 = support_paragraphs.select(\n",
    "        'question',\n",
    "        'row_id',\n",
    "        'paragraph_id',\n",
    "        'title'\n",
    "    )\n",
    "    # paragraphs_df = paragraphs_df.withColumn('paragraph', F.col('title-paragraph').getItem(1)).withColumn('title',F.col('title-paragraph').getItem(0)).drop('title-paragraph')\n",
    "    # df3.select('row_id','paragraph_id','sentence_id','sentence').show(truncate=False)\n",
    "    \n",
    "    title_entities_df=df3.withColumnRenamed('title','text_q')\n",
    "    title_entities_df=pipelineModel.transform(title_entities_df).withColumnRenamed('text_q','title')\n",
    "    \n",
    "    title_entities_df=title_entities_df.select(\n",
    "        F.col('title'),\n",
    "        F.col('row_id'),\n",
    "        F.col('paragraph_id'),\n",
    "        F.col('entities.result').alias('t_entities')\n",
    "    ).dropDuplicates(['row_id','paragraph_id']).withColumn('sentence_id',F.lit(-1))\n",
    "\n",
    "    return title_entities_df\n",
    "\n",
    "\n",
    "def get_split_title_entities_df():\n",
    "    title_entities_df=get_title_entities_df()\n",
    "\n",
    "    split_title_entities_df=title_entities_df.select(\n",
    "        F.col('row_id'),\n",
    "        F.col('paragraph_id'),\n",
    "        F.col('sentence_id'),\n",
    "        F.posexplode(F.col('t_entities')).alias('entity_pos','entity')\n",
    "    ).withColumn('id',F.concat(F.col(\"row_id\"),F.lit(\"-\"), F.col(\"paragraph_id\"),F.lit(\"-\"),F.col('sentence_id'),F.lit('-'),F.col('entity_pos'))).dropDuplicates(['id'])\\\n",
    "    .withColumn('entity_type',F.lit('t'))\n",
    "\n",
    "    return split_title_entities_df\n",
    "    \n",
    "\n",
    "def get_paragraph_level_links_df():\n",
    "    split_title_entities_df=get_split_title_entities_df()\n",
    "\n",
    "    split_sentence_entities_df=get_split_sentence_entities_df()    \n",
    "\n",
    "    paragraph_level_links_df=split_title_entities_df.alias('df1').join(\n",
    "        split_sentence_entities_df.alias('df2'),\n",
    "        (F.col('df1.row_id')==F.col('df2.row_id')) &\n",
    "        (F.col('df1.paragraph_id')==F.col('df2.paragraph_id')) &\n",
    "        (F.col('df1.entity')==F.col('df2.entity')),\n",
    "        'inner'\n",
    "    ).select(\n",
    "        F.col(\"df1.entity\").alias(\"entity1\"),\n",
    "        F.col(\"df2.entity\").alias(\"entity2\"),\n",
    "        F.col('df1.id').alias('src'),\n",
    "        F.col('df2.id').alias('dst'),\n",
    "    )\n",
    "    \n",
    "    return paragraph_level_links_df\n",
    "\n",
    "\n",
    "get_title_entities_df().show(truncate=False)\n",
    "get_split_title_entities_df().show(truncate=False)\n",
    "get_paragraph_level_links_df().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a7721c-451b-4801-a598-9547d604d139",
   "metadata": {},
   "source": [
    "## 4.Title Level Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30fb855c-262d-4b72-a665-623269d379b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 74:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+----------------------+----------+----------+\n",
      "|entity1               |entity2               |src       |dst       |\n",
      "+----------------------+----------------------+----------+----------+\n",
      "|New South Wales       |New South Wales       |125-2--1-0|435-8--1-1|\n",
      "|New South Wales       |New South Wales       |125-2--1-0|174-7--1-1|\n",
      "|California            |California            |128-8--1-1|838-2--1-1|\n",
      "|Avenue                |Avenue                |13-3--1-0 |785-1--1-0|\n",
      "|Numb                  |Numb                  |133-7--1-0|509-4--1-0|\n",
      "|Rihanna               |Rihanna               |133-7--1-1|509-4--1-1|\n",
      "|Texas Tech Red Raiders|Texas Tech Red Raiders|136-3--1-0|662-1--1-0|\n",
      "|Stapleton Cotton      |Stapleton Cotton      |137-5--1-0|934-6--1-0|\n",
      "|Viscount Combermere   |Viscount Combermere   |137-5--1-1|934-6--1-1|\n",
      "|Manchester United F.C |Manchester United F.C |14-5--1-0 |158-0--1-0|\n",
      "|Kingdom               |Kingdom               |147-8--1-0|316-0--1-0|\n",
      "|Kansas                |Kansas                |15-9--1-1 |897-8--1-1|\n",
      "|Kansas                |Kansas                |15-9--1-1 |897-0--1-1|\n",
      "|Kansas                |Kansas                |15-9--1-1 |888-9--1-1|\n",
      "|Kansas                |Kansas                |15-9--1-1 |779-2--1-1|\n",
      "|FIFA World Cup        |FIFA World Cup        |155-4--1-1|161-9--1-1|\n",
      "|FIFA World Cup        |FIFA World Cup        |155-4--1-1|155-6--1-0|\n",
      "|FIFA World Cup        |FIFA World Cup        |155-6--1-0|161-9--1-1|\n",
      "|FIFA World Cup        |FIFA World Cup        |155-6--1-0|155-4--1-1|\n",
      "|Battle of Manila      |Battle of Manila      |156-1--1-0|467-6--1-0|\n",
      "+----------------------+----------------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Connect title entities that appear in other titles,not just in this context\n",
    "# This adds a 'sparse' connection between contexts\n",
    "\n",
    "def get_title_level_links_df():\n",
    "    split_title_entities_df=get_split_title_entities_df()\n",
    "\n",
    "    title_level_links_df=split_title_entities_df.alias('df1').join(\n",
    "        split_title_entities_df.alias('df2'),\n",
    "        (F.col('df1.id')!=F.col('df2.id')) &\n",
    "        (F.col('df1.entity')==F.col('df2.entity')),\n",
    "        'inner'\n",
    "    ).select(\n",
    "        F.col(\"df1.entity\").alias(\"entity1\"),\n",
    "        F.col(\"df2.entity\").alias(\"entity2\"),\n",
    "        F.col('df1.id').alias('src'),\n",
    "        F.col('df2.id').alias('dst'),\n",
    "    )\n",
    "\n",
    "    return title_level_links_df\n",
    "\n",
    "\n",
    "get_title_level_links_df().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bb9d8d-1915-4bb5-9a84-dce79add3091",
   "metadata": {},
   "source": [
    "## 5.Question Level Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "415921d7-5d4a-410c-86f2-66d80365a244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------+-----+----------+------+\n",
      "|question                                                                                                                                               |q_entities                          |level|type      |row_id|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------+-----+----------+------+\n",
      "|Were Scott Derrickson and Ed Wood of the same nationality?                                                                                             |[Scott Derrickson, Ed Wood]         |hard |comparison|0     |\n",
      "|Were Scott Derrickson and Ed Wood of the same nationality?                                                                                             |[Scott Derrickson, Ed Wood]         |hard |comparison|0     |\n",
      "|What government position was held by the woman who portrayed Corliss Archer in the film Kiss and Tell?                                                 |[Corliss Archer, Kiss]              |hard |bridge    |1     |\n",
      "|What government position was held by the woman who portrayed Corliss Archer in the film Kiss and Tell?                                                 |[Corliss Archer, Kiss]              |hard |bridge    |1     |\n",
      "|What science fantasy young adult series, told in first person, has a set of companion books narrating the stories of enslaved worlds and alien species?|[]                                  |hard |bridge    |2     |\n",
      "|What science fantasy young adult series, told in first person, has a set of companion books narrating the stories of enslaved worlds and alien species?|[]                                  |hard |bridge    |2     |\n",
      "|Are the Laleli Mosque and Esma Sultan Mansion located in the same neighborhood?                                                                        |[Laleli Mosque, Esma Sultan Mansion]|hard |comparison|3     |\n",
      "|Are the Laleli Mosque and Esma Sultan Mansion located in the same neighborhood?                                                                        |[Laleli Mosque, Esma Sultan Mansion]|hard |comparison|3     |\n",
      "|The director of the romantic comedy \"Big Stone Gap\" is based in what New York city?                                                                    |[Big Stone Gap, New York]           |hard |bridge    |4     |\n",
      "|The director of the romantic comedy \"Big Stone Gap\" is based in what New York city?                                                                    |[Big Stone Gap, New York]           |hard |bridge    |4     |\n",
      "|2014 S/S is the debut album of a South Korean boy group that was formed by who?                                                                        |[South Korean]                      |hard |bridge    |5     |\n",
      "|2014 S/S is the debut album of a South Korean boy group that was formed by who?                                                                        |[South Korean]                      |hard |bridge    |5     |\n",
      "|Who was known by his stage name Aladin and helped organizations improve their performance as a consultant?                                             |[Aladin]                            |hard |bridge    |6     |\n",
      "|Who was known by his stage name Aladin and helped organizations improve their performance as a consultant?                                             |[Aladin]                            |hard |bridge    |6     |\n",
      "|The arena where the Lewiston Maineiacs played their home games can seat how many people?                                                               |[Lewiston Maineiacs]                |hard |bridge    |7     |\n",
      "|The arena where the Lewiston Maineiacs played their home games can seat how many people?                                                               |[Lewiston Maineiacs]                |hard |bridge    |7     |\n",
      "|Who is older, Annie Morton or Terry Richardson?                                                                                                        |[Annie Morton, Terry Richardson]    |hard |bridge    |8     |\n",
      "|Who is older, Annie Morton or Terry Richardson?                                                                                                        |[Annie Morton, Terry Richardson]    |hard |bridge    |8     |\n",
      "|Are Local H and For Against both from the United States?                                                                                               |[United States]                     |hard |comparison|9     |\n",
      "|Are Local H and For Against both from the United States?                                                                                               |[United States]                     |hard |comparison|9     |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------+-----+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------------------------+-----------+------------+-----------+-----------+\n",
      "|row_id|entity_pos|entity                     |entity_type|paragraph_id|sentence_id|id         |\n",
      "+------+----------+---------------------------+-----------+------------+-----------+-----------+\n",
      "|0     |0         |Scott Derrickson           |q          |-1          |-1         |0--1--1-0  |\n",
      "|0     |1         |Ed Wood                    |q          |-1          |-1         |0--1--1-1  |\n",
      "|1     |0         |Corliss Archer             |q          |-1          |-1         |1--1--1-0  |\n",
      "|1     |1         |Kiss                       |q          |-1          |-1         |1--1--1-1  |\n",
      "|10    |0         |Lawrence                   |q          |-1          |-1         |10--1--1-0 |\n",
      "|10    |1         |Kansas                     |q          |-1          |-1         |10--1--1-1 |\n",
      "|10    |2         |Kansas City                |q          |-1          |-1         |10--1--1-2 |\n",
      "|100   |0         |Indians                    |q          |-1          |-1         |100--1--1-0|\n",
      "|100   |1         |Seminole                   |q          |-1          |-1         |100--1--1-1|\n",
      "|101   |0         |David Huntsinger           |q          |-1          |-1         |101--1--1-0|\n",
      "|102   |0         |Washington Redskins        |q          |-1          |-1         |102--1--1-0|\n",
      "|103   |0         |Georgia Tech Yellow Jackets|q          |-1          |-1         |103--1--1-0|\n",
      "|104   |0         |Fine Gael                  |q          |-1          |-1         |104--1--1-0|\n",
      "|104   |1         |Irish Fine Gael            |q          |-1          |-1         |104--1--1-1|\n",
      "|104   |2         |Taoiseach                  |q          |-1          |-1         |104--1--1-2|\n",
      "|104   |3         |Defence                    |q          |-1          |-1         |104--1--1-3|\n",
      "|105   |0         |American                   |q          |-1          |-1         |105--1--1-0|\n",
      "|105   |1         |Johnny Angel               |q          |-1          |-1         |105--1--1-1|\n",
      "|107   |0         |Warren Buffett             |q          |-1          |-1         |107--1--1-0|\n",
      "|108   |0         |Indian                     |q          |-1          |-1         |108--1--1-0|\n",
      "+------+----------+---------------------------+-----------+------------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+---------------------------+---------+-----------+\n",
      "|entity1                    |entity2                    |src      |dst        |\n",
      "+---------------------------+---------------------------+---------+-----------+\n",
      "|Scott Derrickson           |Scott Derrickson           |0-1-0-0  |0--1--1-0  |\n",
      "|Corliss Archer             |Corliss Archer             |1-6-0-3  |1--1--1-0  |\n",
      "|Kiss                       |Kiss                       |1-6-0-0  |1--1--1-1  |\n",
      "|Lawrence                   |Lawrence                   |10-9-1-3 |10--1--1-0 |\n",
      "|Lawrence                   |Lawrence                   |10-9-1-0 |10--1--1-0 |\n",
      "|Kansas                     |Kansas                     |10-9-1-1 |10--1--1-1 |\n",
      "|Kansas                     |Kansas                     |10-9-0-4 |10--1--1-1 |\n",
      "|Kansas                     |Kansas                     |10-9-0-2 |10--1--1-1 |\n",
      "|Kansas City                |Kansas City                |10-9-2-3 |10--1--1-2 |\n",
      "|Kansas City                |Kansas City                |10-9-2-0 |10--1--1-2 |\n",
      "|Indians                    |Indians                    |100-9-0-1|100--1--1-0|\n",
      "|Seminole                   |Seminole                   |100-9-3-0|100--1--1-1|\n",
      "|Seminole                   |Seminole                   |100-9-1-0|100--1--1-1|\n",
      "|Seminole                   |Seminole                   |100-9-0-2|100--1--1-1|\n",
      "|David Huntsinger           |David Huntsinger           |101-1-0-0|101--1--1-0|\n",
      "|Washington Redskins        |Washington Redskins        |102-3-1-2|102--1--1-0|\n",
      "|Georgia Tech Yellow Jackets|Georgia Tech Yellow Jackets|103-8-1-0|103--1--1-0|\n",
      "|Georgia Tech Yellow Jackets|Georgia Tech Yellow Jackets|103-7-0-0|103--1--1-0|\n",
      "|Fine Gael                  |Fine Gael                  |104-4-0-7|104--1--1-0|\n",
      "|Fine Gael                  |Fine Gael                  |104-4-0-4|104--1--1-0|\n",
      "+---------------------------+---------------------------+---------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add links between the entities of each question to the entities found in the support paragraphs\n",
    "# This connects a question to the respective 'relevant' entities in the paragraphs\n",
    "\n",
    "def get_question_entities_df():\n",
    "    question_entities_df=support_paragraphs.drop('paragraph_id','paragraph','title')\n",
    "\n",
    "    return question_entities_df\n",
    "\n",
    "def get_split_question_entities_df():\n",
    "    question_entities_df=get_question_entities_df()\n",
    "\n",
    "    split_question_entities_df=question_entities_df.select(\n",
    "        F.col('row_id'),\n",
    "        F.posexplode(F.col('q_entities')).alias('entity_pos','entity'),\n",
    "    ).withColumn('entity_type',F.lit('q'))\\\n",
    "    .withColumn('paragraph_id',F.lit(-1))\\\n",
    "    .withColumn('sentence_id',F.lit(-1))\\\n",
    "    .withColumn('id',F.concat(F.col(\"row_id\"),F.lit(\"-\"), F.col(\"paragraph_id\"),F.lit(\"-\"),F.col('sentence_id'),F.lit('-'),F.col('entity_pos'))).dropDuplicates(['id'])\n",
    "\n",
    "    return split_question_entities_df\n",
    "    \n",
    "\n",
    "def get_question_level_links_df():\n",
    "    split_sentence_entities_df=get_split_sentence_entities_df()\n",
    "    split_question_entities_df=get_split_question_entities_df()\n",
    "\n",
    "    question_level_links_df=split_sentence_entities_df.alias('df1').join(\n",
    "        split_question_entities_df.alias('df2'),\n",
    "        (F.col('df1.row_id')==F.col('df2.row_id')) &\n",
    "        (F.col('df1.entity')==F.col('df2.entity')),\n",
    "        'inner'\n",
    "    ).select(\n",
    "        F.col(\"df1.entity\").alias(\"entity1\"),\n",
    "        F.col(\"df2.entity\").alias(\"entity2\"),\n",
    "        F.col('df1.id').alias('src'),\n",
    "        F.col('df2.id').alias('dst'),\n",
    "    )\n",
    "\n",
    "    return question_level_links_df\n",
    "\n",
    "\n",
    "\n",
    "get_question_entities_df().show(truncate=False)\n",
    "get_split_question_entities_df().show(truncate=False)\n",
    "get_question_level_links_df().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8bd4e0-c3d6-4363-9a9e-d059f36facc0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Creating NER Graph(putting all the code together for better efficiency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abefbc53-4eb2-4223-a975-9d7a7bd9a7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After creating a dependency tree,we order the portions of the code accordingly\n",
    "GRAPH_SAVE_FOLDER='/home/ubuntu/graphs/'\n",
    "NER_GRAPH_NAME='NER Graph'\n",
    "\n",
    "\n",
    "def save_NER_graph(graph_name:str,max_no_questions:int):\n",
    "    ##########################\n",
    "    # Create df only with supporting paragraphs\n",
    "    \n",
    "    df1=tokens_and_embeddings_df \n",
    "    \n",
    "    if max_no_questions is not None:\n",
    "        df1=df1.limit(max_no_questions)\n",
    "    \n",
    "    df1=df1.withColumn('row_id',F.monotonically_increasing_id()).withColumnRenamed('result','q_entities')\n",
    "    \n",
    "    paragraphs_df = df1.select(\n",
    "        'question',\n",
    "        'q_entities',\n",
    "        'level',\n",
    "        'type',\n",
    "        'row_id',\n",
    "        F.posexplode(F.col('context')).alias('paragraph_id','title-paragraph')\n",
    "    )\n",
    "    \n",
    "    paragraphs_df = paragraphs_df.withColumn('paragraph', F.col('title-paragraph').getItem(1)).withColumn('title',F.col('title-paragraph').getItem(0)).drop('title-paragraph')\n",
    "    \n",
    "    df2=df1.select('supporting_facts','row_id')\n",
    "    df2=df2.withColumn('supporting_pairs',F.explode(F.col('supporting_facts'))).drop('supporting_facts')\n",
    "    supporting_titles_df=df2.withColumn('supporting_title',F.col('supporting_pairs').getItem(0)).drop('supporting_pairs').dropDuplicates(['supporting_title','row_id'])\n",
    "    \n",
    "    # Drop the paragraphs that are not suportting\n",
    "    support_paragraphs=paragraphs_df.join(supporting_titles_df,(supporting_titles_df.supporting_title==paragraphs_df.title)&(supporting_titles_df.row_id==paragraphs_df.row_id),'inner').select(paragraphs_df['*'])\n",
    "\n",
    "    ##########################\n",
    "    # Get Entities for a sentence as a list\n",
    "\n",
    "    df3=support_paragraphs.drop('q_entities','level','type').withColumn('paragraph_json',F.from_json(F.col('paragraph'), ArrayType(StringType()))).drop('paragraph')\n",
    "    \n",
    "    df3 = df3.select(\n",
    "        'question',\n",
    "        'row_id',\n",
    "        'paragraph_id',\n",
    "         F.posexplode(F.col('paragraph_json')).alias('sentence_id','sentence')\n",
    "    )\n",
    "    \n",
    "    sentence_entities_df=df3.withColumnRenamed('sentence','text_q')\n",
    "    sentence_entities_df=pipelineModel.transform(sentence_entities_df).withColumnRenamed('text_q','sentence').drop('sentence')\n",
    "\n",
    "    sentence_entities_df=sentence_entities_df.select(\n",
    "        F.col('row_id'),\n",
    "        F.col('paragraph_id'),\n",
    "        F.col('sentence_id'),\n",
    "        F.col('entities.result').alias('s_entities')\n",
    "    ).dropDuplicates(['row_id','paragraph_id','sentence_id'])\n",
    "\n",
    "\n",
    "    ##########################\n",
    "    # Get Entities with ids from where they were taken from sentences in paragraphs\n",
    "    split_sentence_entities_df=sentence_entities_df.select(\n",
    "        F.col('row_id'),\n",
    "        F.col('paragraph_id'),\n",
    "        F.col('sentence_id'),\n",
    "        F.posexplode(F.col('s_entities')).alias('entity_pos','entity'),\n",
    "    ).withColumn('id',F.concat(F.col(\"row_id\"),F.lit(\"-\"), F.col(\"paragraph_id\"),F.lit(\"-\"),F.col('sentence_id'),F.lit('-'),F.col('entity_pos'))).dropDuplicates(['id'])\\\n",
    "    .withColumn('entity_type',F.lit('s'))\n",
    "\n",
    "\n",
    "    ##########################\n",
    "    # Get Entities with ids from where they were taken from titles of paragraphs as a list\n",
    "    df3 = support_paragraphs.select(\n",
    "        'question',\n",
    "        'row_id',\n",
    "        'paragraph_id',\n",
    "        'title'\n",
    "    )\n",
    "\n",
    "    title_entities_df=df3.withColumnRenamed('title','text_q')\n",
    "    title_entities_df=pipelineModel.transform(title_entities_df).withColumnRenamed('text_q','title')\n",
    "    \n",
    "    title_entities_df=title_entities_df.select(\n",
    "        F.col('title'),\n",
    "        F.col('row_id'),\n",
    "        F.col('paragraph_id'),\n",
    "        F.col('entities.result').alias('t_entities')\n",
    "    ).dropDuplicates(['row_id','paragraph_id']).withColumn('sentence_id',F.lit(-1))\n",
    "\n",
    "\n",
    "    ##########################\n",
    "    # Get Entities with ids from where they were taken from titles of paragraphs\n",
    "\n",
    "    split_title_entities_df=title_entities_df.select(\n",
    "        F.col('row_id'),\n",
    "        F.col('paragraph_id'),\n",
    "        F.col('sentence_id'),\n",
    "        F.posexplode(F.col('t_entities')).alias('entity_pos','entity')\n",
    "    ).withColumn('id',F.concat(F.col(\"row_id\"),F.lit(\"-\"), F.col(\"paragraph_id\"),F.lit(\"-\"),F.col('sentence_id'),F.lit('-'),F.col('entity_pos'))).dropDuplicates(['id'])\\\n",
    "    .withColumn('entity_type',F.lit('t'))\n",
    "\n",
    "    \n",
    "    ##########################\n",
    "    # Get Sentence Level Links\n",
    "    sentence_level_links_df = (\n",
    "        split_sentence_entities_df.alias(\"df1\")\n",
    "        .join(\n",
    "            split_sentence_entities_df.alias(\"df2\"),\n",
    "            (F.col(\"df1.row_id\") == F.col(\"df2.row_id\")) &  \n",
    "            (F.col(\"df1.paragraph_id\") == F.col(\"df2.paragraph_id\")) & \n",
    "            (F.col(\"df1.sentence_id\") == F.col(\"df2.sentence_id\")) & \n",
    "            (F.col(\"df1.entity\") != F.col(\"df2.entity\")),  # Avoid duplicate and self-pairs\n",
    "            \"inner\"\n",
    "        )\n",
    "        .select(\n",
    "            F.col(\"df1.entity\").alias(\"entity1\"),\n",
    "            F.col(\"df2.entity\").alias(\"entity2\"),\n",
    "            F.col('df1.id').alias('src'),\n",
    "            F.col('df2.id').alias('dst')\n",
    "        )\n",
    "    )\n",
    "\n",
    "    ##########################\n",
    "    # Get Context Level Links\n",
    "    context_level_links_df=split_sentence_entities_df.alias('df1').join(\n",
    "        split_sentence_entities_df.alias('df2'),\n",
    "        (F.col('df1.row_id')==F.col('df2.row_id')) &\n",
    "        (F.col('df1.paragraph_id')!=F.col('df2.paragraph_id')) &\n",
    "        (F.col('df1.entity')==F.col('df2.entity')),\n",
    "        'inner'\n",
    "    ).select(\n",
    "        F.col(\"df1.entity\").alias(\"entity1\"),\n",
    "        # F.col(\"df2.entity\").alias(\"entity2\"),\n",
    "        F.lit('').alias('entity2'),\n",
    "        F.col('df1.id').alias('src'),\n",
    "        F.col('df2.id').alias('dst'),\n",
    "    )\n",
    "\n",
    "    ##########################\n",
    "    # Get Paragraph Level Links\n",
    "    paragraph_level_links_df=split_title_entities_df.alias('df1').join(\n",
    "        split_sentence_entities_df.alias('df2'),\n",
    "        (F.col('df1.row_id')==F.col('df2.row_id')) &\n",
    "        (F.col('df1.paragraph_id')==F.col('df2.paragraph_id')) &\n",
    "        (F.col('df1.entity')==F.col('df2.entity')) &\n",
    "        (F.col('df1.sentence_id')!=F.col('df2.sentence_id')),\n",
    "        'inner'\n",
    "    ).select(\n",
    "        F.col(\"df1.entity\").alias(\"entity1\"),\n",
    "        F.lit('').alias('entity2'),\n",
    "        F.col('df1.id').alias('src'),\n",
    "        F.col('df2.id').alias('dst'),\n",
    "    )\n",
    "\n",
    "    ##########################\n",
    "    # Get Title Level Links\n",
    "\n",
    "    title_level_links_df=split_title_entities_df.alias('df1').join(\n",
    "        split_title_entities_df.alias('df2'),\n",
    "        (F.col('df1.id')!=F.col('df2.id')) &\n",
    "        (F.col('df1.entity')==F.col('df2.entity')),\n",
    "        'inner'\n",
    "    ).select(\n",
    "        F.col(\"df1.entity\").alias(\"entity1\"),\n",
    "        F.col(\"df2.entity\").alias(\"entity2\"),\n",
    "        F.col('df1.id').alias('src'),\n",
    "        F.col('df2.id').alias('dst'),\n",
    "    )\n",
    "\n",
    "    ##########################\n",
    "    # Get Question Level Links\n",
    "\n",
    "    question_entities_df=support_paragraphs.drop('paragraph_id','paragraph','title')\n",
    "\n",
    "    split_question_entities_df=question_entities_df.select(\n",
    "        F.col('row_id'),\n",
    "        F.posexplode(F.col('q_entities')).alias('entity_pos','entity'),\n",
    "    ).withColumn('entity_type',F.lit('q'))\\\n",
    "    .withColumn('paragraph_id',F.lit(-1))\\\n",
    "    .withColumn('sentence_id',F.lit(-1))\\\n",
    "    .withColumn('id',F.concat(F.col(\"row_id\"),F.lit(\"-\"), F.col(\"paragraph_id\"),F.lit(\"-\"),F.col('sentence_id'),F.lit('-'),F.col('entity_pos'))).dropDuplicates(['id'])\n",
    "    \n",
    "    question_level_links_df=split_sentence_entities_df.alias('df1').join(\n",
    "        split_question_entities_df.alias('df2'),\n",
    "        (F.col('df1.row_id')==F.col('df2.row_id')) &\n",
    "        (F.col('df1.entity')==F.col('df2.entity')),\n",
    "        'inner'\n",
    "    ).select(\n",
    "        F.col(\"df1.entity\").alias(\"entity1\"),\n",
    "        F.col(\"df2.entity\").alias(\"entity2\"),\n",
    "        F.col('df1.id').alias('src'),\n",
    "        F.col('df2.id').alias('dst'),\n",
    "    )\n",
    "\n",
    "    # Create and save graph\n",
    "    \n",
    "    vertices=split_sentence_entities_df.union(split_title_entities_df).union(split_question_entities_df)\n",
    "    edges=sentence_level_links_df.union(context_level_links_df).union(paragraph_level_links_df).union(title_level_links_df).union(question_level_links_df)\n",
    "\n",
    "    vertices_path = GRAPH_SAVE_FOLDER +NER_GRAPH_NAME + graph_name + ' Vertices'\n",
    "    edges_path = GRAPH_SAVE_FOLDER +NER_GRAPH_NAME + graph_name + ' Edges'\n",
    "\n",
    "    vertices.write.parquet(vertices_path)\n",
    "    edges.write.parquet(edges_path)\n",
    "\n",
    "    print(f'{graph_name} with max {max_no_questions} questions')\n",
    "    print(f'No nodes: {vertices.count()}')\n",
    "    print(f'No edges: {edges.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "170c5e26-a4dd-4f44-b561-614f122558f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " f3 with max None questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No nodes: 171020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No edges: 638296\n",
      "Execution time  383.3619194030762\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time=time.time()\n",
    "# save_NER_graph(max_no_questions=None,graph_name=' f3')\n",
    "\n",
    "execution_time=time.time()-start_time\n",
    "print(\"Execution time \",execution_time)\n",
    "\n",
    "# f3->Graph for dev with Title and Question entities\n",
    "# f3 with max None questions\n",
    "                                                                                \n",
    "# No nodes: 171020                                                                       \n",
    "# No edges: 638296"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480b35ad-cd89-47aa-8885-71d34bd8c0e2",
   "metadata": {},
   "source": [
    "## Load NER Graph and save interesting subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e14ace3d-1cee-435d-99a4-a485618852f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "GRAPH_SAVE_FOLDER='/home/ubuntu/graphs/'\n",
    "NER_GRAPH_NAME='NER Graph'\n",
    "\n",
    "GRAPH_NODE_SIZE = 5\n",
    "\n",
    "PAGE_RANK_NODE_SCALING_SIZE = 5\n",
    "AS_LENGTH_EDGE = 500\n",
    "\n",
    "OUTPUT_FILE_FOLDER='results/Louvain/NER Subgraphs dev'\n",
    "\n",
    "def create_txt_for_subgraph(subgraph, component_id: int):\n",
    "    no_vertices = subgraph.vertices.count()\n",
    "    no_edges = subgraph.edges.count()\n",
    "    subgraph_name = f\"Component {component_id} - {no_vertices}.txt\"\n",
    "\n",
    "    directory = OUTPUT_FILE_FOLDER  + '/'\n",
    "    # Create folders if they don't exist\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    file_path = directory + subgraph_name\n",
    "\n",
    "    start_index = 1\n",
    "    answer_index = dict()\n",
    "\n",
    "    with codecs.open(file_path, 'w', \"utf-8\") as file:\n",
    "        file.write(f'p {no_vertices} {no_edges}\\n')\n",
    "\n",
    "        for row in subgraph.vertices.collect():\n",
    "            node_id = row['id']\n",
    "\n",
    "            if node_id not in answer_index:\n",
    "                answer_index[node_id] = start_index\n",
    "                start_index += 1\n",
    "\n",
    "            entity,entity_type =  row['entity'],row['entity_type']\n",
    "            entity_type=('Sentence' if entity_type=='s' else ('Title' if entity_type=='t' else 'Question'))\n",
    "        \n",
    "            node_index = answer_index[node_id]\n",
    "\n",
    "            file.write(f'v {node_index} {entity} {entity_type}\\n')\n",
    "\n",
    "        for row in subgraph.edges.collect():\n",
    "            node_id1 = row['src']\n",
    "            node_id2 = row['dst']\n",
    "\n",
    "            file.write(f'e {answer_index[node_id1]} {answer_index[node_id2]} 1\\n')\n",
    "\n",
    "def create_html_for_subgraph(subgraph, component_id: int):\n",
    "    no_vertices = subgraph.vertices.count()\n",
    "\n",
    "    nx_graph = nx.empty_graph()\n",
    "    for row in subgraph.vertices.collect():\n",
    "        node_id = row['id']\n",
    "\n",
    "        entity,entity_type =  row['entity'],row['entity_type']\n",
    "        entity_type=('Sentence' if entity_type=='s' else ('Title' if entity_type=='t' else 'Question'))\n",
    "\n",
    "        page_rank = row['pagerank']\n",
    "        group = row['label']\n",
    "\n",
    "        label = f'Entity:{entity}\\nType:{entity_type}\\nPage Rank:{page_rank:.2f}\\nLabel:{group}'\n",
    "        shape = 'square'\n",
    "\n",
    "        node_size = int(round(PAGE_RANK_NODE_SCALING_SIZE * page_rank))\n",
    "\n",
    "        nx_graph.add_node(node_id, size=node_size, title=label, label=label, shape=shape, group=group)\n",
    "\n",
    "    for edge in subgraph.edges.collect():\n",
    "        node1 = edge['src']\n",
    "        node2 = edge['dst']\n",
    "        weight = edge['weight']\n",
    "\n",
    "        nx_graph.add_edge(node1, node2, label=weight)\n",
    "\n",
    "    nt = Network('900px', '1700px', select_menu=True, cdn_resources='remote')\n",
    "    nt.from_nx(nx_graph)\n",
    "\n",
    "    for edge in subgraph.edges.collect():\n",
    "        node1 = edge['src']\n",
    "        node2 = edge['dst']\n",
    "        nx_graph.add_edge(node1, node2, length=AS_LENGTH_EDGE)\n",
    "\n",
    "    nt = Network('900px', '1700px', select_menu=True, cdn_resources='remote')\n",
    "    nt.from_nx(nx_graph)\n",
    "\n",
    "    # nt.enable_physics(True)\n",
    "\n",
    "    nt.set_options(\"\"\"\n",
    "                    {\n",
    "                      \"interaction\": {\n",
    "                        \"hover\": true,          \n",
    "                        \"navigationButtons\": true,\n",
    "                        \"keyboard\": true        \n",
    "                      },\n",
    "                      \"manipulation\": {\n",
    "                        \"enabled\": true         \n",
    "                      },\n",
    "                      \"physics\": {\n",
    "                        \"enabled\": true,\n",
    "                        \"repulsion\": {\n",
    "                          \"nodeDistance\": 450,\n",
    "                          \"springLength\": 500\n",
    "                        }\n",
    "                      },\n",
    "                      \"layout\": {\n",
    "                        \"hierarchical\": {\n",
    "                          \"enabled\": false\n",
    "                        }\n",
    "                      }\n",
    "                    }\n",
    "                    \"\"\"\n",
    "                   )\n",
    "\n",
    "    html_name = f\"Component {component_id} - {no_vertices} nodes.html\"\n",
    "    html_directory = OUTPUT_FILE_FOLDER + '/'\n",
    "    # Create folders if they don't exist\n",
    "    if not os.path.exists(html_directory):\n",
    "        os.makedirs(html_directory)\n",
    "\n",
    "    html_path = html_directory + html_name\n",
    "\n",
    "    html = nt.generate_html()\n",
    "    with open(html_path, mode='w',\n",
    "              encoding='utf-8') as html_file:\n",
    "        html_file.write(html)\n",
    "    # display(HTML(html))\n",
    "\n",
    "\n",
    "def show_ner_subgraphs(subgraphs_min_no_nodes: int = 4, subgraphs_max_no_nodes=20,\n",
    "                       subgraphs_min_no_edges: int = 4,\n",
    "                         page_rank_beta: float = 0.15, page_rank_max_no_iterations: int = 10,\n",
    "                         lpa_max_no_iterations: int = 10,\n",
    "                         verbose_console: bool = True,\n",
    "                         graph=None):\n",
    "    df_connected_components = graph.connectedComponents(algorithm=\"graphx\")\n",
    "\n",
    "    # df_connected_components.show()\n",
    "\n",
    "    df_connected_components_grouped = df_connected_components.groupBy(\"component\")\n",
    "\n",
    "    df_connected_components_sizes = df_connected_components_grouped.agg(F.count(\"*\").alias(\"size\"))\n",
    "\n",
    "    df_connected_components_max_min = df_connected_components_sizes.agg(F.max(F.col('size')).alias('max_no_nodes'),\n",
    "                                                                        F.min(F.col('size')).alias(\n",
    "                                                                            'min_no_nodes')).first()\n",
    "    max_size_connected_component, min_size_connected_component = df_connected_components_max_min[0], \\\n",
    "                                                                 df_connected_components_max_min[1]\n",
    "\n",
    "    if verbose_console:\n",
    "        print(f\"Max no nodes in a component: {max_size_connected_component}\")\n",
    "        print(f\"Min no nodes in a component: {min_size_connected_component}\")\n",
    "\n",
    "    df_connected_components = df_connected_components.select(['id', 'component'])\n",
    "\n",
    "    # df_max_components = df_connected_components_sizes.filter(F.col(\"size\") == max_size_connected_component).select(\n",
    "    #     \"component\").collect()\n",
    "\n",
    "    for component in df_connected_components_sizes.toLocalIterator():\n",
    "        component_id, component_size = component['component'], component['size']\n",
    "\n",
    "        if component_size < subgraphs_min_no_nodes or component_size > subgraphs_max_no_nodes:\n",
    "            continue\n",
    "\n",
    "        # print(f\"Nodes for the component {component_id}\")\n",
    "\n",
    "        subgraph_vertices = graph.vertices.join(df_connected_components,\n",
    "                                                on='id').filter(\n",
    "            F.col('component') == component_id).dropDuplicates(['id'])\n",
    "\n",
    "        # TODO:Attempt more efficient way to get the nodes and edges...\n",
    "        # subgraph_vertices.show(truncate=False)\n",
    "        # subgraph = graph.filterVertices(F.col('id')).dropIsolatedVertices()\n",
    "\n",
    "        vertices_ids_src = subgraph_vertices.select('id').withColumnRenamed('id', 'src')\n",
    "        vertices_ids_dst = subgraph_vertices.select('id').withColumnRenamed('id', 'dst')\n",
    "\n",
    "        subgraph_edges = graph.edges.join(vertices_ids_src, on='src', how='inner').join(vertices_ids_dst, on='dst',\n",
    "                                                                                        how='inner')\n",
    "\n",
    "        no_edges = subgraph_edges.count()\n",
    "        if no_edges < subgraphs_min_no_edges:\n",
    "            continue\n",
    "\n",
    "        subgraph = GraphFrame(subgraph_vertices, subgraph_edges)\n",
    "        labels_df = subgraph.labelPropagation(maxIter=lpa_max_no_iterations).select(['id', 'label'])\n",
    "\n",
    "        subgraph_vertices = subgraph_vertices.join(labels_df, on='id')\n",
    "        subgraph = GraphFrame(subgraph_vertices, subgraph_edges)\n",
    "\n",
    "        subgraph = subgraph.pageRank(resetProbability=page_rank_beta, maxIter=page_rank_max_no_iterations)\n",
    "\n",
    "        create_html_for_subgraph(subgraph, component_id)\n",
    "        create_txt_for_subgraph(subgraph,component_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bab0385-c947-46f0-9b4a-dcee79c19df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # dataset_folder=DATASETS_FOLDER\n",
    "    # dataset_name='hotpot_train_v1.1'\n",
    "    \n",
    "    full_graph_folder='/home/ubuntu/graphs/'\n",
    "\n",
    "    # Full graph obtained from dev dataset\n",
    "    full_graph_name='NER Graph f3'\n",
    "\n",
    "    graph=load_graph(full_graph_folder,full_graph_name)\n",
    "    \n",
    "    show_ner_subgraphs(subgraphs_min_no_nodes=4, subgraphs_max_no_nodes=100,\n",
    "                       subgraphs_min_no_edges=4,\n",
    "                         page_rank_beta=0.15, page_rank_max_no_iterations=50,\n",
    "                         lpa_max_no_iterations=35,\n",
    "                         verbose_console=True,\n",
    "                         graph=graph)\n",
    "    \n",
    "    print(f\"Execution time: {time.time() - start_time} s\")\n",
    "\n",
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1d42849-3eb6-4ec0-b7ad-699b3d8d73ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max no nodes in a component: 14399\n",
      "Min no nodes in a component: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 23\u001b[0m\n\u001b[1;32m     14\u001b[0m     show_ner_subgraphs(subgraphs_min_no_nodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, subgraphs_max_no_nodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m     15\u001b[0m                        subgraphs_min_no_edges\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     16\u001b[0m                          page_rank_beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.15\u001b[39m, page_rank_max_no_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     17\u001b[0m                          lpa_max_no_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     18\u001b[0m                          verbose_console\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     19\u001b[0m                          graph\u001b[38;5;241m=\u001b[39mgraph)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m s\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m \u001b[43mmain2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m, in \u001b[0;36mmain2\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m full_graph_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNER Graph f3\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     12\u001b[0m graph\u001b[38;5;241m=\u001b[39mload_graph(full_graph_folder,full_graph_name)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mshow_ner_subgraphs\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubgraphs_min_no_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs_max_no_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                   \u001b[49m\u001b[43msubgraphs_min_no_edges\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpage_rank_beta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpage_rank_max_no_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mlpa_max_no_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mverbose_console\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m s\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 192\u001b[0m, in \u001b[0;36mshow_ner_subgraphs\u001b[0;34m(subgraphs_min_no_nodes, subgraphs_max_no_nodes, subgraphs_min_no_edges, page_rank_beta, page_rank_max_no_iterations, lpa_max_no_iterations, verbose_console, graph)\u001b[0m\n\u001b[1;32m    189\u001b[0m subgraph_vertices \u001b[38;5;241m=\u001b[39m subgraph_vertices\u001b[38;5;241m.\u001b[39mjoin(labels_df, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    190\u001b[0m subgraph \u001b[38;5;241m=\u001b[39m GraphFrame(subgraph_vertices, subgraph_edges)\n\u001b[0;32m--> 192\u001b[0m subgraph \u001b[38;5;241m=\u001b[39m \u001b[43msubgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpageRank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresetProbability\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpage_rank_beta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxIter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpage_rank_max_no_iterations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m create_html_for_subgraph(subgraph, component_id)\n\u001b[1;32m    195\u001b[0m create_txt_for_subgraph(subgraph,component_id)\n",
      "File \u001b[0;32m/tmp/spark-a412e654-8ad5-431e-9097-2b1421ab99f2/userFiles-35b9eb68-42e3-4a57-97bf-332ad86eb13f/graphframes_graphframes-0.8.4-spark3.5-s_2.12.jar/graphframes/graphframe.py:371\u001b[0m, in \u001b[0;36mGraphFrame.pageRank\u001b[0;34m(self, resetProbability, sourceId, maxIter, tol)\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m tol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExactly one of maxIter or tol should be set.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    370\u001b[0m     builder \u001b[38;5;241m=\u001b[39m builder\u001b[38;5;241m.\u001b[39mtol(tol)\n\u001b[0;32m--> 371\u001b[0m jgf \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _from_java_gf(jgf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "def main2():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # dataset_folder=DATASETS_FOLDER\n",
    "    # dataset_name='hotpot_train_v1.1'\n",
    "    \n",
    "    full_graph_folder='/home/ubuntu/graphs/'\n",
    "\n",
    "    # Full graph obtained from dev dataset\n",
    "    full_graph_name='NER Graph f3'\n",
    "\n",
    "    graph=load_graph(full_graph_folder,full_graph_name)\n",
    "    \n",
    "    show_ner_subgraphs(subgraphs_min_no_nodes=100, subgraphs_max_no_nodes=1000,\n",
    "                       subgraphs_min_no_edges=4,\n",
    "                         page_rank_beta=0.15, page_rank_max_no_iterations=1,\n",
    "                         lpa_max_no_iterations=1,\n",
    "                         verbose_console=True,\n",
    "                         graph=graph)\n",
    "    \n",
    "    print(f\"Execution time: {time.time() - start_time} s\")\n",
    "\n",
    "main2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651f32d1-2660-4002-8821-7e1b6abb0670",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
