{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95bc7e9c-ca0d-4ccf-b836-5584e9693904",
   "metadata": {},
   "source": [
    "# Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1c614f-3efa-411d-a0dd-fe1fe37e54a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeGraph(n, edges, clusters, n_colors=10, nodes_q=None, title='default'):\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(edges)\n",
    "    G.add_nodes_from(range(1, n + 1))\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, n_colors))\n",
    "    colors = [mcolors.to_hex(color) for color in colors]\n",
    "    colors_assign_community = {comm: colors[comm] for comm in set(clusters.values())}\n",
    "    pos = nx.spring_layout(G)  # Use spring layout (force-directed)\n",
    "    edge_trace = []\n",
    "    for edge in G.edges():\n",
    "        x0, y0 = pos[edge[0]]\n",
    "        x1, y1 = pos[edge[1]]\n",
    "        edge_trace.append(\n",
    "            go.Scatter(\n",
    "                x=[x0, x1, None], y=[y0, y1, None],\n",
    "                line=dict(width=0.5, color=\"#888\"),\n",
    "                hoverinfo=\"none\",\n",
    "                mode=\"lines\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Create node traces grouped by community\n",
    "    node_traces = []\n",
    "    for community, color in colors_assign_community.items():\n",
    "        community_nodes = [node for node in G.nodes if clusters[node] == community]\n",
    "        node_x = [pos[node][0] for node in community_nodes]\n",
    "        node_y = [pos[node][1] for node in community_nodes]\n",
    "\n",
    "        if nodes_q is None:\n",
    "            node_traces.append(\n",
    "                go.Scatter(\n",
    "                    x=node_x, y=node_y,\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(size=40, color=color, line=dict(width=1)),\n",
    "                    text=[f\"Node {node}\" for node in community_nodes],\n",
    "                    hoverinfo=\"text\"\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "                node_traces.append(\n",
    "                    go.Scatter(\n",
    "                        x=node_x, y=node_y,\n",
    "                        mode=\"markers\",\n",
    "                        marker=dict(size=40, color=color, line=dict(width=1)),\n",
    "                        text=[nodes_q[node - 1] for node in community_nodes],\n",
    "                        hoverinfo=\"text\"\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    # Combine traces\n",
    "    fig = go.Figure(data=edge_trace + node_traces)\n",
    "    fig.update_layout(\n",
    "        showlegend=False,\n",
    "        hovermode=\"closest\",\n",
    "        margin=dict(b=0, l=0, r=0, t=0),\n",
    "        xaxis=dict(showgrid=False, zeroline=False),\n",
    "        yaxis=dict(showgrid=False, zeroline=False)\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "    fig.write_html(f\"./{title}.html\")\n",
    "    return colors_assign_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f130391c-26b3-441b-9e38-95aeb697e7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/17 02:47:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "        .appName(\"HotpotQA Clustering\")\n",
    "        .config('spark.executor.instances','2')\n",
    "        .config('spark.executor.memory','12G')\n",
    "        .config(\"spark.driver.memory\", \"4G\")\n",
    "        .config('spark.executor.cores','6')\n",
    "        .config('spark.dynamicAllocation.enabled','false') # musai??\n",
    "        .master('spark://master:7077')\n",
    "        .getOrCreate())\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc6a4af8-cefd-4460-84d3-e61669efc95d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/jupyter/results/Clusterization Results\n",
      "Number of nodes: 129\n",
      "Number of edges: 459\n",
      "[(3, -0.010793569424865083), (15, -0.010793569424865083), (31, -0.010793569424865083), (33, -0.010793569424865083), (39, -0.010822048499864727), (43, -0.010793569424865082), (49, -0.010793569424865082), (53, -0.010935964799863302), (74, -0.010935964799863303), (78, -0.010793569424865083), (106, -0.010594215899867572), (120, -0.010793569424865077)]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 123\u001b[0m\n\u001b[1;32m    121\u001b[0m     favourite_config \u001b[38;5;241m=\u001b[39m favourite_config\u001b[38;5;241m.\u001b[39mcollect()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 123\u001b[0m     favourite_config \u001b[38;5;241m=\u001b[39m \u001b[43mfavourite_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(favourite_config) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m:\n\u001b[1;32m    125\u001b[0m         favourite_config \u001b[38;5;241m=\u001b[39m favourite_config[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/rdd.py:1926\u001b[0m, in \u001b[0;36mRDD.reduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   1924\u001b[0m vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmapPartitions(func)\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vals:\n\u001b[0;32m-> 1926\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1927\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not reduce() empty RDD\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/util.py:83\u001b[0m, in \u001b[0;36mfail_on_stopiteration.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[1;32m     86\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTOP_ITERATION_OCCURRED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     87\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     88\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexc\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(exc),\n\u001b[1;32m     89\u001b[0m             },\n\u001b[1;32m     90\u001b[0m         )\n",
      "Cell \u001b[0;32mIn[4], line 123\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(key, value)\u001b[0m\n\u001b[1;32m    121\u001b[0m     favourite_config \u001b[38;5;241m=\u001b[39m favourite_config\u001b[38;5;241m.\u001b[39mcollect()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 123\u001b[0m     favourite_config \u001b[38;5;241m=\u001b[39m favourite_config\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28;01mlambda\u001b[39;00m key, value: \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(favourite_config) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m:\n\u001b[1;32m    125\u001b[0m         favourite_config \u001b[38;5;241m=\u001b[39m favourite_config[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "def initialize_communities(num_nodes):\n",
    "    all_nodes = [i for i in range(1, num_nodes + 1)]\n",
    "    all_nodes = sc.parallelize(all_nodes)\n",
    "    communities = all_nodes.map(lambda x_: (x_, x_)).collectAsMap()  # x = node\n",
    "    return communities\n",
    "\n",
    "\n",
    "def compute_modularity():\n",
    "    communities = communities_broadcast.value\n",
    "    A = bc_A.value\n",
    "    k = bc_k.value\n",
    "    total_weight = bc_total_weight.value\n",
    "    node_pairs = sc.parallelize(bc_node_pairs.value)\n",
    "\n",
    "    same_community = node_pairs.map(lambda e: (e[0], e[1], communities[e[0]], communities[e[1]])) \\\n",
    "        .filter(lambda x_: x_[2] == x_[3]) \\\n",
    "        .map(lambda x_: A.get((x_[0], x_[1]), 0) - (1 / (2 * total_weight)) * k[x_[0]] * k[x_[1]]) \\\n",
    "        .sum()\n",
    "    return (1 / (2 * total_weight)) * same_community\n",
    "\n",
    "\n",
    "def compute_delta_modularity(node, paired):\n",
    "    bc_communities = communities_broadcast.value\n",
    "    A = bc_A.value\n",
    "    k = bc_k.value\n",
    "    total_weight = bc_total_weight.value\n",
    "    node_pairs = bc_node_pairs.value\n",
    "    if bc_communities[node] != bc_communities[paired]:\n",
    "        bc_communities[node] = bc_communities[paired]\n",
    "\n",
    "    same_community = 0.0\n",
    "    for e in node_pairs:\n",
    "        if bc_communities[e[0]] == bc_communities[e[1]]:\n",
    "            same_community += A.get((e[0], e[1]), 0) - (1 / (2 * total_weight)) * k[e[0]] * k[e[1]]\n",
    "    mod = (1 / (2 * total_weight)) * same_community\n",
    "    improved = False\n",
    "    if bc_modularity.value < mod:\n",
    "        improved = True\n",
    "    return paired, mod, improved\n",
    "\n",
    "\n",
    "old_dir = os.getcwd()\n",
    "print(old_dir)\n",
    "INPUT_PATH = '/home/ubuntu/data/graph-AS-129.txt'\n",
    "IT_COUNT = 4\n",
    "\n",
    "precision = 1e-6\n",
    "data = sc.textFile(INPUT_PATH)\n",
    "os.chdir('/home/ubuntu/jupyter/results/Clusterization Results')\n",
    "metadata = data.zipWithIndex() \\\n",
    "    .filter(lambda x_: x_[1] == 0) \\\n",
    "    .map(lambda line: [info for info in line[0].split(' ')]) \\\n",
    "    .collect()\n",
    "n = int(metadata[0][1])\n",
    "m = int(metadata[0][2])\n",
    "n_broadcast = sc.broadcast(n)\n",
    "print(f\"Number of nodes: {n}\")\n",
    "print(f\"Number of edges: {m}\")\n",
    "edges = data.zipWithIndex() \\\n",
    "    .filter(lambda x_: n < x_[1] < n + m + 1) \\\n",
    "    .map(lambda line: [coordinate for coordinate in line[0].split(' ')[1:]]) \\\n",
    "    .map(lambda line: [(int(line[0]), int(line[1])), float(line[2])])\n",
    "nodes = data.zipWithIndex() \\\n",
    "    .filter(lambda x_: 0 < x_[1] < n + 1) \\\n",
    "    .map(lambda line: \" \".join(line[0].split(' ')[2:]))\n",
    "nodes_q = nodes.collect()\n",
    "# coordinates = data.zipWithIndex() \\\n",
    "#     .filter(lambda x: x[1] > m) \\\n",
    "#     .map(lambda line: [float(coordinate) for coordinate in line[0].split(' ')[1:]]) \\\n",
    "#     .collect()\n",
    "# artificially extend dataset with self-loops\n",
    "node_pairs = [((i, i), 0) for i in range(1, n + 1)]\n",
    "self_loops = sc.parallelize(node_pairs)\n",
    "edges = edges.union(self_loops)\n",
    "# persist data\n",
    "edges.persist()\n",
    "\n",
    "for it in range(IT_COUNT):\n",
    "    total_weight = edges.map(lambda e: e[1]).sum()\n",
    "    bc_total_weight = sc.broadcast(total_weight)\n",
    "    node_pairs_vec = [(i, j) for i in range(1, n + 1) for j in range(1, n + 1)]\n",
    "    bc_node_pairs = sc.broadcast(node_pairs_vec)\n",
    "    A = edges.flatMap(lambda e: (((e[0][0], e[0][1]), e[1]), ((e[0][1], e[0][0]), e[1]))).collectAsMap()\n",
    "    k = edges.flatMap(lambda e: [(e[0][0], e[1]), (e[0][1], e[1])]) \\\n",
    "        .reduceByKey(lambda x_, y: x_ + y).collectAsMap()\n",
    "    for node in range(1, n + 1):\n",
    "        if node not in k.keys():\n",
    "            k[node] = 0\n",
    "    for pair in node_pairs_vec:\n",
    "        if pair not in A.keys():\n",
    "            A[pair] = 0.0\n",
    "    bc_A = sc.broadcast(A)\n",
    "    bc_k = sc.broadcast(k)\n",
    "    new_communities = initialize_communities(n)\n",
    "    # print(new_communities)\n",
    "    communities_broadcast = sc.broadcast(new_communities)\n",
    "    modularity = compute_modularity()\n",
    "    bc_modularity = sc.broadcast(modularity)\n",
    "    # print(modularity)\n",
    "\n",
    "    # form communities from each node, compute modularity, choose best\n",
    "    improvement = True\n",
    "    global_improvement = False\n",
    "    adjacency_list = edges.flatMap(lambda e: [(e[0][0], (e[0][0], e[0][1])), (e[0][1], (e[0][1], e[0][0]))]) \\\n",
    "        .groupByKey() \\\n",
    "        .collectAsMap()\n",
    "    while improvement:\n",
    "        improvement = False\n",
    "        for node, pairs in adjacency_list.items():\n",
    "            pairs = sc.parallelize(pairs)\n",
    "            favourite_config = pairs.filter(lambda x_: x_[0] != x_[1]) \\\n",
    "                .map(lambda x_: compute_delta_modularity(x_[0], x_[1])) \\\n",
    "                .filter(lambda res: res[2] is True) \\\n",
    "                .map(lambda res: (res[0], res[1]))\n",
    "            if not favourite_config.isEmpty():\n",
    "                favourite_config = favourite_config.reduce(lambda key, value: max(value))\n",
    "                if type(favourite_config) is tuple:\n",
    "                    favourite_config = favourite_config[0]\n",
    "                improvement = True\n",
    "                global_improvement = True\n",
    "                new_communities[node] = new_communities[favourite_config]\n",
    "                communities_broadcast = sc.broadcast(new_communities)\n",
    "                modularity = compute_modularity()\n",
    "                bc_modularity = sc.broadcast(modularity)\n",
    "\n",
    "    print(\"Here 1\")\n",
    "    # transition to clusters to provide input for visualization\n",
    "\n",
    "    # change graph (by making use of communities)\n",
    "    # change broadcast values\n",
    "    new_edges = edges.map(lambda edge: ((new_communities[edge[0][0]], new_communities[edge[0][1]]), edge[1])) \\\n",
    "        .reduceByKey(lambda a, b: a + b)\n",
    "    self_edges = new_edges.filter(lambda edge: edge[0][0] == edge[0][1]) \\\n",
    "        .map(lambda e: ((e[0][0], e[0][1]), 2 * e[1]))\n",
    "    other_edges = new_edges.filter(lambda edge: edge[0][0] != edge[0][1])\n",
    "    new_edges = other_edges.union(self_edges)\n",
    "    new_nodes = new_edges.flatMap(lambda edge: [edge[0][0], edge[0][1]]).distinct()\n",
    "    new_nodes_collection = new_nodes.collect()\n",
    "    n_old = n\n",
    "    n = new_nodes.count()\n",
    "    n_broadcast = sc.broadcast(n)\n",
    "    mapping = {}\n",
    "    for index, x in enumerate(new_nodes_collection):\n",
    "        mapping[x] = index + 1\n",
    "    mapping = sc.broadcast(mapping).value\n",
    "\n",
    "    # TODO: !!!!!!!!!!!!!!!!!\n",
    "    _edges = edges.map(lambda e: (e[0][0], e[0][1])).collect()\n",
    "    # visualizeGraph(n_old, _edges, new_communities)\n",
    "    if it == 0:\n",
    "        visualizeGraph(n_old, _edges, {key: mapping[value] for key, value in new_communities.items()},\n",
    "                       nodes_q=nodes_q, title=f'{it}_AS_Louvain_1')\n",
    "    else:\n",
    "        visualizeGraph(n_old, _edges, {key: mapping[value] for key, value in new_communities.items()},\n",
    "                      title=f'{it}_AS_Louvain_1')\n",
    "\n",
    "    if not global_improvement:\n",
    "        break\n",
    "\n",
    "    edges = new_edges.map(lambda edge: ((mapping[edge[0][0]],\n",
    "                                         mapping[edge[0][1]]),\n",
    "                                        edge[1]))\n",
    "    edges.persist()\n",
    "\n",
    "    print(\"Here 2\")\n",
    "    # transition to clusters to provide input for visualization\n",
    "    # TODO: !!!!!!!!!!!!!!!!!\n",
    "    _edges = edges.map(lambda e: (e[0][0], e[0][1])).collect()\n",
    "    # visualizeGraph(n, _edges, {(i + 1): value for i, value in enumerate(new_nodes_collection)})\n",
    "    visualizeGraph(n, _edges, {(i + 1): (i + 1) for i in range(n)}, title=f'{it}_AS_Louvain_2')\n",
    "\n",
    "os.chdir(old_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4725a940-5233-4eb2-a0f3-67fb0367f645",
   "metadata": {},
   "source": [
    "# Same code works on my machine, on the same datasets...and gives valid results which are saved in /results/Clusterization Results/Louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4dcb79-b28b-4697-ab34-3b34c8d98581",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
